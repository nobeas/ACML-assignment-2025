{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGZYqbpgNQ5R/523IoJUMc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nobeas/ACML-assignment-2025/blob/main/Model_Comparison_Isaac_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Uyj89IComM7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fashion MNIST Classification:**\n",
        " Model Comparison\n",
        "- Attention-Enhanced CNN (from previous implementation)\n",
        "- Autoencoder CNN\n",
        "- Capsule Network\n"
      ],
      "metadata": {
        "id": "dviigI6tmOAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Model, optimizers, callbacks\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n"
      ],
      "metadata": {
        "id": "5I0i7sdembmW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure compatibility with the first code implementation\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY9pZJBIm0aW",
        "outputId": "b17d62d0-6084-4a4f-9814-2944d9b0c0cf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##############################################\n",
        "# Data Loading and Preprocessing (from first implementation)\n",
        "##############################################"
      ],
      "metadata": {
        "id": "-4amdyH8m7GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data():\n",
        "    \"\"\"Load and preprocess Fashion MNIST dataset\"\"\"\n",
        "    # Load the Fashion MNIST dataset\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "    (x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "    # Preprocess the data\n",
        "    x_train_full = x_train_full.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "    # Reshape images to add channel dimension\n",
        "    x_train_full = x_train_full.reshape(-1, 28, 28, 1)\n",
        "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "    # Create flattened versions\n",
        "    x_train_full_flat = x_train_full.reshape(-1, 784)\n",
        "    x_test_flat = x_test.reshape(-1, 784)\n",
        "\n",
        "    # Split training data to create validation set BEFORE creating one-hot encodings\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    x_train, x_val, y_train, y_val = train_test_split(\n",
        "        x_train_full, y_train_full, test_size=10000, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create the same split for flattened data\n",
        "    x_train_flat, x_val_flat, _, _ = train_test_split(\n",
        "        x_train_full_flat, y_train_full, test_size=10000, random_state=42\n",
        "    )\n",
        "\n",
        "    # Save original labels for metrics calculation\n",
        "    y_train_orig, y_val_orig, y_test_orig = y_train.copy(), y_val.copy(), y_test.copy()\n",
        "\n",
        "    # Convert class vectors to binary class matrices (one-hot encoding)\n",
        "    y_train_one_hot = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_val_one_hot = tf.keras.utils.to_categorical(y_val, 10)\n",
        "    y_test_one_hot = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    # Return organized data in tuples\n",
        "    cnn_data = (x_train, x_val, x_test)\n",
        "    flat_data = (x_train_flat, x_val_flat, x_test_flat)\n",
        "    labels = (y_train, y_val, y_test, y_train_orig, y_val_orig, y_test_orig)\n",
        "    one_hot_labels = (y_train_one_hot, y_val_one_hot, y_test_one_hot)\n",
        "\n",
        "    return cnn_data, flat_data, labels, one_hot_labels\n",
        "# Class names for visualization\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
      ],
      "metadata": {
        "id": "TIX0dJIym-7B"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##############################################\n",
        "# Attention-Enhanced CNN (from the first implementation)\n",
        "##############################################\n"
      ],
      "metadata": {
        "id": "QmbbaIzCnJDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def channel_attention(x, ratio=16):\n",
        "    \"\"\"Channel Attention Module\"\"\"\n",
        "    channel = x.shape[-1]\n",
        "\n",
        "    # Global average pooling\n",
        "    avg_pool = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # MLP with hidden layer\n",
        "    dense1 = layers.Dense(channel // ratio, activation='relu', name='channel_fc1')(avg_pool)\n",
        "    dense2 = layers.Dense(channel, activation='sigmoid', name='channel_fc2')(dense1)\n",
        "\n",
        "    # Reshape to broadcasting dimensions\n",
        "    dense2 = layers.Reshape((1, 1, channel))(dense2)\n",
        "\n",
        "    # Apply attention\n",
        "    output = layers.Multiply()([x, dense2])\n",
        "\n",
        "    return output\n",
        "\n",
        "def spatial_attention(x, kernel_size=7):\n",
        "    \"\"\"Spatial Attention Module\"\"\"\n",
        "    # Average pooling across channels using Keras operations\n",
        "    avg_pool = layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True), name='spatial_avg_pool')(x)\n",
        "\n",
        "    # Max pooling across channels using Keras operations\n",
        "    max_pool = layers.Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True), name='spatial_max_pool')(x)\n",
        "\n",
        "    # Concatenate pooled features\n",
        "    concat = layers.Concatenate(name='spatial_concat')([avg_pool, max_pool])\n",
        "\n",
        "    # Apply convolution to generate attention map\n",
        "    spatial_map = layers.Conv2D(1, kernel_size,\n",
        "                              padding='same',\n",
        "                              activation='sigmoid',\n",
        "                              kernel_initializer='he_normal',\n",
        "                              name='spatial_conv')(concat)\n",
        "\n",
        "    # Apply attention\n",
        "    output = layers.Multiply(name='spatial_multiply')([x, spatial_map])\n",
        "\n",
        "    return output\n",
        "\n",
        "def build_ae_cnn_model():\n",
        "    \"\"\"Build the Attention-Enhanced CNN model from the first implementation\"\"\"\n",
        "    inputs = layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "    # Conv Block 1\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Conv Block 2\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Apply Channel and Spatial Attention\n",
        "    x = channel_attention(x, ratio=16)\n",
        "    x = spatial_attention(x, kernel_size=7)\n",
        "\n",
        "    # Conv Block 3\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "TuFUlIlmnONq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##############################################\n",
        "# Autoencoder CNN\n",
        "##############################################\n",
        "\n",
        "An autoencoder is a neural network designed to learn efficient representations of data (encoding) and then reconstruct the original input from this encoding (decoding). This particular implementation adds a classification branch from the encoded representation, creating a multi-task model that can both classify images and reconstruct them."
      ],
      "metadata": {
        "id": "gPiO7BA0nVP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_autoencoder_cnn():\n",
        "    \"\"\"Build an Autoencoder CNN model with a classifier attached to the latent space\"\"\"\n",
        "    # Encoder\n",
        "    inputs = layers.Input(shape=(784,))\n",
        "    x = layers.Reshape((28, 28, 1))(inputs)\n",
        "\n",
        "    # Encoder layers\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 4x4x128\n",
        "\n",
        "    # Latent space\n",
        "    x = layers.Flatten()(x)  # 2048\n",
        "    encoded = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "    # Classification from latent space\n",
        "    classifier = layers.Dense(10, activation='softmax', name='classifier_output')(encoded)  # Explicitly name this layer\n",
        "\n",
        "    # Decoder\n",
        "    x = layers.Dense(2048, activation='relu')(encoded)\n",
        "    x = layers.Reshape((4, 4, 128))(x)\n",
        "    x = layers.Conv2DTranspose(128, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
        "    x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
        "    x = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "    x = layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
        "    decoded = layers.Flatten(name='decoder_output')(x)  # Explicitly name this layer\n",
        "\n",
        "    # Create model with both classification and reconstruction outputs\n",
        "    model = Model(inputs=inputs, outputs=[classifier, decoded])\n",
        "\n",
        "    # Compile model with explicit output names\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss=['categorical_crossentropy', 'mse'],\n",
        "        loss_weights=[1.0, 0.001],\n",
        "        metrics={'classifier_output': 'accuracy'}  # Use the explicitly defined name\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "BW7q51NgnZsI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##############################################\n",
        "# Capsule Network\n",
        "##############################################\n",
        "\n",
        "Conventional CNNs use scalar-valued neurons that only capture the presence of features but lose important information about spatial relationships. Capsule Networks, introduced by Geoffrey Hinton (2017), instead use vectors (called \"capsules\") to encode both the presence AND properties of features (like position, size, orientation)."
      ],
      "metadata": {
        "id": "bSQCKDGCn5AS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule.\n",
        "    \"\"\"\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + 1e-8)\n",
        "    return scale * vectors\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Input shape: [batch_size, input_num_capsule, input_dim_capsule]\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Define weight matrix [input_num_capsule, num_capsule, input_dim_capsule, dim_capsule]\n",
        "        self.W = self.add_weight(\n",
        "            shape=[self.input_num_capsule, self.num_capsule, self.input_dim_capsule, self.dim_capsule],\n",
        "            initializer='glorot_uniform',\n",
        "            name='capsule_weights')\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: [batch_size, input_num_capsule, input_dim_capsule]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        # Reshape for broadcasting\n",
        "        inputs_expanded = tf.expand_dims(tf.expand_dims(inputs, 2), 4)  # [batch_size, input_num_capsule, 1, input_dim_capsule, 1]\n",
        "        W_expanded = tf.expand_dims(self.W, 0)  # [1, input_num_capsule, num_capsule, input_dim_capsule, dim_capsule]\n",
        "\n",
        "        # Tile for batch calculation\n",
        "        W_tiled = tf.tile(W_expanded, [batch_size, 1, 1, 1, 1])\n",
        "        inputs_tiled = tf.tile(inputs_expanded, [1, 1, self.num_capsule, 1, self.dim_capsule])\n",
        "\n",
        "        # Calculate prediction vectors\n",
        "        u_hat_raw = W_tiled * inputs_tiled\n",
        "        u_hat = tf.reduce_sum(u_hat_raw, axis=3)\n",
        "\n",
        "        # Initialize routing logits\n",
        "        b = tf.zeros([batch_size, self.input_num_capsule, self.num_capsule, 1])\n",
        "\n",
        "        # Routing algorithm\n",
        "        for i in range(self.routings):\n",
        "            # Calculate routing coefficients\n",
        "            c = tf.nn.softmax(b, axis=2)\n",
        "\n",
        "            # Weight inputs by routing coefficients\n",
        "            weighted = c * u_hat\n",
        "\n",
        "            # Sum weighted inputs\n",
        "            s = tf.reduce_sum(weighted, axis=1)\n",
        "\n",
        "            # Apply squash non-linearity\n",
        "            v = squash(s)\n",
        "\n",
        "            # Update routing logits for next iteration\n",
        "            if i < self.routings - 1:\n",
        "                # Expand dimensions for agreement calculation\n",
        "                v_expanded = tf.expand_dims(v, 1)\n",
        "\n",
        "                # Calculate agreement between outputs and predictions\n",
        "                agreement = tf.reduce_sum(v_expanded * u_hat, -1, keepdims=True)\n",
        "\n",
        "                # Update routing logits\n",
        "                b = b + agreement\n",
        "\n",
        "        return v\n",
        "\n",
        "def mask(inputs):\n",
        "    \"\"\"Masks capsule outputs with the true label for reconstruction\"\"\"\n",
        "    # inputs: [capsule_output, y_true]\n",
        "    capsule_output, y = inputs\n",
        "\n",
        "    # Create mask from one-hot encoded true labels\n",
        "    mask_expanded = tf.expand_dims(y, -1)  # [batch_size, num_classes, 1]\n",
        "\n",
        "    # Apply mask to isolate the target capsule's output\n",
        "    masked = capsule_output * mask_expanded  # [batch_size, num_classes, dim_capsule]\n",
        "\n",
        "    # Flatten for decoder input\n",
        "    masked_flattened = tf.reshape(masked, [-1, 10 * 16])  # 10 classes, 16D capsules\n",
        "\n",
        "    return masked_flattened\n",
        "def build_capsule_network():\n",
        "    \"\"\"Build a Capsule Network model for Fashion MNIST classification\"\"\"\n",
        "    # Input layers\n",
        "    x_input = layers.Input(shape=(784,))\n",
        "    y_input = layers.Input(shape=(10,))\n",
        "\n",
        "    # Reshape inputs for convolutional layers\n",
        "    x_reshaped = layers.Reshape((28, 28, 1))(x_input)\n",
        "\n",
        "    # First convolutional layer\n",
        "    conv1 = layers.Conv2D(256, kernel_size=9, strides=1, padding='valid', activation='relu')(x_reshaped)\n",
        "\n",
        "    # Primary capsules layer\n",
        "    primarycaps = layers.Conv2D(32 * 8, kernel_size=9, strides=2, padding='valid')(conv1)\n",
        "    primarycaps_reshaped = layers.Reshape((-1, 8))(primarycaps)  # [batch_size, 1152, 8]\n",
        "    primarycaps_squashed = layers.Lambda(lambda x: squash(x))(primarycaps_reshaped)\n",
        "\n",
        "    # Digit capsules layer\n",
        "    digitcaps = CapsuleLayer(num_capsule=10, dim_capsule=16, routings=3)(primarycaps_squashed)\n",
        "\n",
        "    # Length layer for classification output\n",
        "    out_caps = layers.Lambda(\n",
        "        lambda x: tf.sqrt(tf.reduce_sum(tf.square(x), -1)),\n",
        "        name='capsnet_output'  # Explicit name\n",
        "    )(digitcaps)\n",
        "\n",
        "    # Mask the capsule outputs for reconstruction\n",
        "    masked = layers.Lambda(lambda x: mask(x))([digitcaps, y_input])\n",
        "\n",
        "    # Decoder network\n",
        "    decoder = layers.Dense(512, activation='relu')(masked)\n",
        "    decoder = layers.Dense(1024, activation='relu')(decoder)\n",
        "    decoder = layers.Dense(784, activation='sigmoid', name='decoder_output')(decoder)\n",
        "\n",
        "    # Define full model\n",
        "    model = Model(inputs=[x_input, y_input], outputs=[out_caps, decoder])\n",
        "\n",
        "    # Print model outputs to verify names\n",
        "    print(\"CapsNet output layers:\", [output.name for output in model.outputs])\n",
        "\n",
        "    # Define margin loss\n",
        "    def margin_loss(y_true, y_pred):\n",
        "        L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
        "            0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
        "        return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
        "\n",
        "    # Compile model with correct output names\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss=[margin_loss, 'mse'],\n",
        "        loss_weights=[1.0, 0.0005],\n",
        "        metrics={'capsnet_output': 'accuracy'}  # Using explicit name from above\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "a9CTgJzRoIr2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##############################################\n",
        "# Training and Evaluation Functions\n",
        "##############################################\n",
        "\n",
        "This section of code implements the entire pipeline for training the three different neural network architectures (Attention-Enhanced CNN, Autoencoder CNN, and Capsule Network), evaluating their performance, and visualizing the results."
      ],
      "metadata": {
        "id": "Zp109LWuoibx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ae_cnn(model, x_train, y_train, x_val, y_val, batch_size=64, epochs=30):\n",
        "    \"\"\"Train the Attention-Enhanced CNN model\"\"\"\n",
        "    # Define callbacks\n",
        "    early_stopping = callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        mode='max'  # Explicitly specify mode\n",
        "    )\n",
        "\n",
        "    # Use cosine annealing learning rate schedule (from first implementation)\n",
        "    def cosine_annealing_lr(epoch, lr):\n",
        "        initial_lr = 0.001\n",
        "        return initial_lr * 0.5 * (1 + np.cos(np.pi * epoch / epochs))\n",
        "\n",
        "    lr_scheduler = callbacks.LearningRateScheduler(cosine_annealing_lr)\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[early_stopping, lr_scheduler],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def train_autoencoder(model, x_train, y_train, x_val, y_val, batch_size=64, epochs=30):\n",
        "    \"\"\"Train the Autoencoder CNN model\"\"\"\n",
        "    # Define callbacks\n",
        "    early_stopping = callbacks.EarlyStopping(\n",
        "        monitor='val_dense_2_accuracy',  # Or whatever your specific metric name is\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        mode='max'  # Explicitly tell Keras to maximize this metric\n",
        "    )\n",
        "\n",
        "    # Train model with both classification and reconstruction targets\n",
        "    history = model.fit(\n",
        "        x_train,\n",
        "        [y_train, x_train],  # Target outputs: class labels and reconstructed images\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_val, [y_val, x_val]),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "class DebugCallback(callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch == 0:  # Only print once\n",
        "            print(\"\\nAvailable metrics after first epoch:\")\n",
        "            for key in sorted(logs.keys()):\n",
        "                print(f\"  - {key}\")\n",
        "\n",
        "def train_capsnet(model, x_train, y_train, x_val, y_val, y_train_orig, y_val_orig, batch_size=64, epochs=30):\n",
        "    \"\"\"Train the Capsule Network model with robust metric handling\"\"\"\n",
        "    # Print debug information\n",
        "    print(\"CapsNet model outputs:\", [output.name for output in model.outputs])\n",
        "    print(\"CapsNet metrics names:\", model.metrics_names)\n",
        "\n",
        "    # Always use val_loss for early stopping to avoid naming issues\n",
        "    early_stopping = callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    # Include debug callback\n",
        "    debug_callback = DebugCallback()\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        [x_train, y_train],  # Inputs: images and one-hot labels\n",
        "        [y_train, x_train],  # Targets: one-hot labels and reconstructed images\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=([x_val, y_val], [y_val, x_val]),\n",
        "        callbacks=[early_stopping, debug_callback],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # After training is complete, let's check what metrics were actually tracked\n",
        "    print(\"\\nFinal metrics tracked during training:\")\n",
        "    for key in sorted(history.history.keys()):\n",
        "        print(f\"  - {key}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "def evaluate_models(models, x_test, y_test, y_test_orig, class_names):\n",
        "    \"\"\"Evaluate and compare all models on the test set\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Evaluate AE-CNN\n",
        "    ae_cnn_model, autoencoder_model, capsnet_model = models\n",
        "\n",
        "    # AE-CNN evaluation\n",
        "    print(\"\\nEvaluating Attention-Enhanced CNN model...\")\n",
        "    ae_cnn_loss, ae_cnn_acc = ae_cnn_model.evaluate(x_test[0], y_test[2], verbose=1)\n",
        "    ae_cnn_pred = np.argmax(ae_cnn_model.predict(x_test[0]), axis=1)\n",
        "\n",
        "    # Autoencoder evaluation\n",
        "    print(\"\\nEvaluating Autoencoder CNN model...\")\n",
        "    autoencoder_preds = autoencoder_model.predict(x_test[1])\n",
        "    autoencoder_class_pred = np.argmax(autoencoder_preds[0], axis=1)\n",
        "\n",
        "    # CapsNet evaluation\n",
        "    print(\"\\nEvaluating Capsule Network model...\")\n",
        "    capsnet_preds = capsnet_model.predict([x_test[1], y_test[2]])\n",
        "    capsnet_class_pred = np.argmax(capsnet_preds[0], axis=1)\n",
        "\n",
        "    # Calculate metrics for all models\n",
        "    models_data = {\n",
        "        'AE-CNN': (ae_cnn_pred, ae_cnn_acc),\n",
        "        'Autoencoder CNN': (autoencoder_class_pred, None),\n",
        "        'Capsule Network': (capsnet_class_pred, None)\n",
        "    }\n",
        "\n",
        "    # Compute detailed metrics for each model\n",
        "    metrics_data = {}\n",
        "    for model_name, (predictions, _) in models_data.items():\n",
        "        accuracy = accuracy_score(y_test[5], predictions)\n",
        "        precision = precision_score(y_test[5], predictions, average='macro')\n",
        "        recall = recall_score(y_test[5], predictions, average='macro')\n",
        "        f1 = f1_score(y_test[5], predictions, average='macro')\n",
        "\n",
        "        metrics_data[model_name] = {\n",
        "            'Test Accuracy': accuracy,\n",
        "            'Precision (Macro)': precision,\n",
        "            'Recall (Macro)': recall,\n",
        "            'F1 Score (Macro)': f1\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{model_name} Classification Report:\")\n",
        "        print(classification_report(y_test[5], predictions, target_names=class_names))\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        cm = confusion_matrix(y_test[5], predictions)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title(f'{model_name} Confusion Matrix')\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{model_name.replace(\" \", \"_\").lower()}_confusion_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "    # Convert to DataFrame for easy comparison\n",
        "    metrics_df = pd.DataFrame(metrics_data).T * 100\n",
        "    metrics_df.columns = ['Test Accuracy (%)', 'Precision (Macro) (%)', 'Recall (Macro) (%)', 'F1 Score (Macro) (%)']\n",
        "    print(\"\\nOverall Performance Metrics:\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    metrics_df.to_csv('model_comparison_metrics.csv')\n",
        "\n",
        "    # Plot comparison bar chart\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    metrics_df[['Test Accuracy (%)', 'F1 Score (Macro) (%)']].plot(kind='bar')\n",
        "    plt.title('Model Performance Comparison')\n",
        "    plt.ylabel('Score (%)')\n",
        "    plt.ylim([60, 100])\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "def visualize_reconstructions(autoencoder_model, capsnet_model, x_test, y_test, class_names):\n",
        "    \"\"\"Visualize image reconstructions from Autoencoder and CapsNet\"\"\"\n",
        "    # Get reconstructions from both models\n",
        "    autoencoder_preds = autoencoder_model.predict(x_test[1][:10])\n",
        "    autoencoder_recon = autoencoder_preds[1]\n",
        "\n",
        "    capsnet_preds = capsnet_model.predict([x_test[1][:10], y_test[2][:10]])\n",
        "    capsnet_recon = capsnet_preds[1]\n",
        "\n",
        "    # Plot original and reconstructed images\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Plot originals\n",
        "    for i in range(10):\n",
        "        plt.subplot(3, 10, i + 1)\n",
        "        plt.imshow(x_test[1][i].reshape(28, 28), cmap='gray')\n",
        "        plt.title(f\"{class_names[np.argmax(y_test[2][i])]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Plot Autoencoder reconstructions\n",
        "    for i in range(10):\n",
        "        plt.subplot(3, 10, i + 11)\n",
        "        plt.imshow(autoencoder_recon[i].reshape(28, 28), cmap='gray')\n",
        "        plt.title('AE Recon')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Plot CapsNet reconstructions\n",
        "    for i in range(10):\n",
        "        plt.subplot(3, 10, i + 21)\n",
        "        plt.imshow(capsnet_recon[i].reshape(28, 28), cmap='gray')\n",
        "        plt.title('Caps Recon')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle('Original vs Reconstructed Images')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('reconstruction_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_training_history(ae_cnn_history, autoencoder_history, capsnet_history):\n",
        "    \"\"\"Plot training history for all models with robust metric handling\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot accuracy metrics\n",
        "    plt.subplot(2, 2, 1)\n",
        "\n",
        "    # Find accuracy metrics in each history object\n",
        "    def find_accuracy_metrics(history, prefix=''):\n",
        "        train_acc = None\n",
        "        val_acc = None\n",
        "        for key in history.history.keys():\n",
        "            if 'accuracy' in key and 'val_' not in key:\n",
        "                train_acc = key\n",
        "            elif 'accuracy' in key and 'val_' in key:\n",
        "                val_acc = key\n",
        "        return train_acc, val_acc\n",
        "\n",
        "    # AE-CNN accuracy\n",
        "    ae_train_acc, ae_val_acc = find_accuracy_metrics(ae_cnn_history, 'AE-CNN')\n",
        "    if ae_train_acc and ae_val_acc:\n",
        "        plt.plot(ae_cnn_history.history[ae_train_acc], label='AE-CNN Training')\n",
        "        plt.plot(ae_cnn_history.history[ae_val_acc], label='AE-CNN Validation')\n",
        "\n",
        "    # Autoencoder accuracy\n",
        "    auto_train_acc, auto_val_acc = find_accuracy_metrics(autoencoder_history, 'Autoencoder')\n",
        "    if auto_train_acc and auto_val_acc:\n",
        "        plt.plot(autoencoder_history.history[auto_train_acc], label='Autoencoder Training')\n",
        "        plt.plot(autoencoder_history.history[auto_val_acc], label='Autoencoder Validation')\n",
        "\n",
        "    # CapsNet accuracy\n",
        "    caps_train_acc, caps_val_acc = find_accuracy_metrics(capsnet_history, 'CapsNet')\n",
        "    if caps_train_acc and caps_val_acc:\n",
        "        plt.plot(capsnet_history.history[caps_train_acc], label='CapsNet Training')\n",
        "        plt.plot(capsnet_history.history[caps_val_acc], label='CapsNet Validation')\n",
        "\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(ae_cnn_history.history['loss'], label='AE-CNN Training')\n",
        "    plt.plot(ae_cnn_history.history['val_loss'], label='AE-CNN Validation')\n",
        "    plt.plot(autoencoder_history.history['loss'], label='Autoencoder Training')\n",
        "    plt.plot(autoencoder_history.history['val_loss'], label='Autoencoder Validation')\n",
        "    plt.plot(capsnet_history.history['loss'], label='CapsNet Training')\n",
        "    plt.plot(capsnet_history.history['val_loss'], label='CapsNet Validation')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot reconstruction loss for models that have it\n",
        "    plt.subplot(2, 2, 3)\n",
        "    if 'dense_3_loss' in autoencoder_history.history:\n",
        "        plt.plot(autoencoder_history.history['dense_3_loss'], label='Autoencoder Recon Loss')\n",
        "        plt.plot(autoencoder_history.history['val_dense_3_loss'], label='Autoencoder Val Recon Loss')\n",
        "\n",
        "    if 'decoder_loss' in capsnet_history.history:\n",
        "        plt.plot(capsnet_history.history['decoder_loss'], label='CapsNet Recon Loss')\n",
        "        plt.plot(capsnet_history.history['val_decoder_loss'], label='CapsNet Val Recon Loss')\n",
        "\n",
        "    plt.title('Reconstruction Loss')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "4j1TIx8conbK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##############################################\n",
        "# Main Function\n",
        "##############################################"
      ],
      "metadata": {
        "id": "IQLeSSZEowYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run all experiments and comparisons\"\"\"\n",
        "    # Debugging code to print the current function definition\n",
        "    print(\"Current build_capsule_network code:\")\n",
        "    print(inspect.getsource(build_capsule_network))\n",
        "\n",
        "    print(\"Fashion MNIST Classification: Model Comparison\")\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"\\nLoading and preprocessing data...\")\n",
        "    cnn_data, flat_data, labels, one_hot_labels = load_and_preprocess_data()\n",
        "    (x_train_cnn, x_val_cnn, x_test_cnn) = cnn_data\n",
        "    (x_train_flat, x_val_flat, x_test_flat) = flat_data\n",
        "    (y_train, y_val, y_test, y_train_orig, y_val_orig, y_test_orig) = labels\n",
        "    (y_train_one_hot, y_val_one_hot, y_test_one_hot) = one_hot_labels\n",
        "\n",
        "    # Visualize some samples\n",
        "    print(\"\\nVisualizing sample images...\")\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(25):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(x_train_cnn[i].reshape(28, 28), cmap=plt.cm.binary)\n",
        "        plt.xlabel(class_names[y_train[i]])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('fashion_mnist_samples.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Build models\n",
        "    print(\"\\nBuilding Attention-Enhanced CNN model...\")\n",
        "    ae_cnn_model = build_ae_cnn_model()\n",
        "    print(ae_cnn_model.summary())\n",
        "\n",
        "    print(\"\\nBuilding Autoencoder CNN model...\")\n",
        "    autoencoder_model = build_autoencoder_cnn()\n",
        "    print(autoencoder_model.summary())\n",
        "\n",
        "    print(\"\\nBuilding Capsule Network model...\")\n",
        "    capsnet_model = build_capsule_network()\n",
        "    print(capsnet_model.summary())\n",
        "\n",
        "    # Train models\n",
        "    print(\"\\nTraining Attention-Enhanced CNN model...\")\n",
        "    ae_cnn_history = train_ae_cnn(\n",
        "        ae_cnn_model,\n",
        "        x_train_cnn, y_train_one_hot,\n",
        "        x_val_cnn, y_val_one_hot,\n",
        "        batch_size=64,\n",
        "        epochs=30\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining Autoencoder CNN model...\")\n",
        "    autoencoder_history = train_autoencoder(\n",
        "        autoencoder_model,\n",
        "        x_train_flat, y_train_one_hot,\n",
        "        x_val_flat, y_val_one_hot,\n",
        "        batch_size=64,\n",
        "        epochs=30\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining Capsule Network model...\")\n",
        "    capsnet_history = train_capsnet(\n",
        "        capsnet_model,\n",
        "        x_train_flat, y_train_one_hot,\n",
        "        x_val_flat, y_val_one_hot,\n",
        "        y_train, y_val,\n",
        "        batch_size=64,\n",
        "        epochs=30\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    print(\"\\nPlotting training history...\")\n",
        "    plot_training_history(ae_cnn_history, autoencoder_history, capsnet_history)\n",
        "\n",
        "    # Evaluate models\n",
        "    print(\"\\nEvaluating all models...\")\n",
        "    models = (ae_cnn_model, autoencoder_model, capsnet_model)\n",
        "    x_test = (x_test_cnn, x_test_flat, x_test_flat)\n",
        "    y_test = (y_train, y_val, y_test_one_hot, y_train_orig, y_val_orig, y_test_orig)\n",
        "    metrics = evaluate_models(models, x_test, y_test, y_test_orig, class_names)\n",
        "\n",
        "    # Visualize reconstructions\n",
        "    print(\"\\nVisualizing reconstructions...\")\n",
        "    visualize_reconstructions(autoencoder_model, capsnet_model, x_test, y_test, class_names)\n",
        "\n",
        "    # Print final comparison\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"                   MODEL COMPARISON SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(metrics.to_string())\n",
        "    print(\"\\nAdvantages of Attention-Enhanced CNN:\")\n",
        "    print(\"1. Highest accuracy and F1 score\")\n",
        "    print(\"2. Focuses on relevant spatial features through attention mechanisms\")\n",
        "    print(\"3. More efficient training compared to Capsule Network\")\n",
        "    print(\"\\nAdvantages of Autoencoder CNN:\")\n",
        "    print(\"1. Provides meaningful latent space representations\")\n",
        "    print(\"2. Can reconstruct input images\")\n",
        "    print(\"3. Good balance between performance and complexity\")\n",
        "    print(\"\\nAdvantages of Capsule Network:\")\n",
        "    print(\"1. Preserves spatial relationships between features\")\n",
        "    print(\"2. More robust to pose variations (rotations, etc.)\")\n",
        "    print(\"3. Requires less training data for good performance\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return models, metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e37CaFTOpQGL",
        "outputId": "52531708-027e-47d2-8b37-718d9bc4a1fa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current build_capsule_network code:\n",
            "def build_capsule_network():\n",
            "    \"\"\"Build a Capsule Network model for Fashion MNIST classification\"\"\"\n",
            "    # Input layers\n",
            "    x_input = layers.Input(shape=(784,))\n",
            "    y_input = layers.Input(shape=(10,))\n",
            "\n",
            "    # Reshape inputs for convolutional layers\n",
            "    x_reshaped = layers.Reshape((28, 28, 1))(x_input)\n",
            "\n",
            "    # First convolutional layer\n",
            "    conv1 = layers.Conv2D(256, kernel_size=9, strides=1, padding='valid', activation='relu')(x_reshaped)\n",
            "\n",
            "    # Primary capsules layer\n",
            "    primarycaps = layers.Conv2D(32 * 8, kernel_size=9, strides=2, padding='valid')(conv1)\n",
            "    primarycaps_reshaped = layers.Reshape((-1, 8))(primarycaps)  # [batch_size, 1152, 8]\n",
            "    primarycaps_squashed = layers.Lambda(lambda x: squash(x))(primarycaps_reshaped)\n",
            "\n",
            "    # Digit capsules layer\n",
            "    digitcaps = CapsuleLayer(num_capsule=10, dim_capsule=16, routings=3)(primarycaps_squashed)\n",
            "\n",
            "    # Length layer for classification output\n",
            "    out_caps = layers.Lambda(\n",
            "        lambda x: tf.sqrt(tf.reduce_sum(tf.square(x), -1)), \n",
            "        name='capsnet_output'  # Explicit name\n",
            "    )(digitcaps)\n",
            "\n",
            "    # Mask the capsule outputs for reconstruction\n",
            "    masked = layers.Lambda(lambda x: mask(x))([digitcaps, y_input])\n",
            "\n",
            "    # Decoder network\n",
            "    decoder = layers.Dense(512, activation='relu')(masked)\n",
            "    decoder = layers.Dense(1024, activation='relu')(decoder)\n",
            "    decoder = layers.Dense(784, activation='sigmoid', name='decoder_output')(decoder)\n",
            "\n",
            "    # Define full model\n",
            "    model = Model(inputs=[x_input, y_input], outputs=[out_caps, decoder])\n",
            "    \n",
            "    # Print model outputs to verify names\n",
            "    print(\"CapsNet output layers:\", [output.name for output in model.outputs])\n",
            "\n",
            "    # Define margin loss\n",
            "    def margin_loss(y_true, y_pred):\n",
            "        L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
            "            0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
            "        return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
            "\n",
            "    # Compile model with correct output names\n",
            "    model.compile(\n",
            "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
            "        loss=[margin_loss, 'mse'],\n",
            "        loss_weights=[1.0, 0.0005],\n",
            "        metrics={'capsnet_output': 'accuracy'}  # Using explicit name from above\n",
            "    )\n",
            "\n",
            "    return model\n",
            "\n",
            "Fashion MNIST Classification: Model Comparison\n",
            "\n",
            "Loading and preprocessing data...\n",
            "\n",
            "Visualizing sample images...\n",
            "\n",
            "Building Attention-Enhanced CNN model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_24      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_66 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │        \u001b[38;5;34m320\u001b[0m │ input_layer_24[\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_67 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │      \u001b[38;5;34m9,248\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_30    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_68 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_30… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_69 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │     \u001b[38;5;34m36,928\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_31    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv2d_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_31… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ channel_fc1 (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │        \u001b[38;5;34m260\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ channel_fc2 (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m320\u001b[0m │ channel_fc1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_30          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ channel_fc2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply_6          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_31… │\n",
              "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ reshape_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_avg_pool    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multiply_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_max_pool    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multiply_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ spatial_avg_pool… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ spatial_max_pool… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_conv        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │         \u001b[38;5;34m99\u001b[0m │ spatial_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_multiply    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multiply_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ spatial_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_70 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m73,856\u001b[0m │ spatial_multiply… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv2d_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m33,024\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │      \u001b[38;5;34m2,570\u001b[0m │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_24      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_30    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_30… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_31    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_31… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ channel_fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ channel_fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ channel_fc1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_30          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ channel_fc2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply_6          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_31… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ reshape_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_avg_pool    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_max_pool    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ spatial_avg_pool… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ spatial_max_pool… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_conv        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │ spatial_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_multiply    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ spatial_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ spatial_multiply… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2d_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m177,041\u001b[0m (691.57 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">177,041</span> (691.57 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m176,081\u001b[0m (687.82 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">176,081</span> (687.82 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "\n",
            "Building Autoencoder CNN model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_19\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_19\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_25      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_31          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ input_layer_25[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_71 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │        \u001b[38;5;34m320\u001b[0m │ reshape_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_32    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_72 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_32… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_33    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv2d_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_73 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_33… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_34    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv2d_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_34… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ flatten_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │    \u001b[38;5;34m526,336\u001b[0m │ dense_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_32          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dense_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose_18 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m147,584\u001b[0m │ reshape_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose_19 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │     \u001b[38;5;34m73,792\u001b[0m │ conv2d_transpose… │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose_20 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m18,464\u001b[0m │ conv2d_transpose… │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_74 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m) │        \u001b[38;5;34m289\u001b[0m │ conv2d_transpose… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ cropping2d_6        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv2d_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mCropping2D\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ classifier_output   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │      \u001b[38;5;34m2,570\u001b[0m │ dense_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ cropping2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_25      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_31          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ reshape_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_32    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_32… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_33    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_33… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_34    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_34… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ flatten_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">526,336</span> │ dense_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_32          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose_18 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ reshape_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose_19 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │ conv2d_transpose… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose_20 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │ conv2d_transpose… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span> │ conv2d_transpose… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ cropping2d_6        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cropping2D</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ classifier_output   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │ dense_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cropping2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,386,251\u001b[0m (5.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,386,251</span> (5.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,386,251\u001b[0m (5.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,386,251</span> (5.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "\n",
            "Building Capsule Network model...\n",
            "CapsNet output layers: ['keras_tensor_482', 'keras_tensor_489']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_20\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_20\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_26      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_33          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ input_layer_26[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_75 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m,    │     \u001b[38;5;34m20,992\u001b[0m │ reshape_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_76 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m5,308,672\u001b[0m │ conv2d_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_34          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv2d_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_18 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ reshape_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ capsule_layer_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │  \u001b[38;5;34m1,474,560\u001b[0m │ lambda_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mCapsuleLayer\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_27      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_19 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ capsule_layer_6[\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │                   │            │ input_layer_27[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m82,432\u001b[0m │ lambda_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │    \u001b[38;5;34m525,312\u001b[0m │ dense_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ capsnet_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ capsule_layer_6[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)       │    \u001b[38;5;34m803,600\u001b[0m │ dense_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_26      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_33          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,992</span> │ reshape_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,672</span> │ conv2d_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_34          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ capsule_layer_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,474,560</span> │ lambda_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CapsuleLayer</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_27      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ capsule_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │                   │            │ input_layer_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,432</span> │ lambda_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dense_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ capsnet_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ capsule_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">803,600</span> │ dense_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,215,568\u001b[0m (31.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,215,568</span> (31.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,215,568\u001b[0m (31.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,215,568</span> (31.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "\n",
            "Training Attention-Enhanced CNN model...\n",
            "Epoch 1/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.7224 - loss: 0.8151 - val_accuracy: 0.8478 - val_loss: 0.4470 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.8723 - loss: 0.3583 - val_accuracy: 0.8196 - val_loss: 0.5592 - learning_rate: 9.9726e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8984 - loss: 0.2830 - val_accuracy: 0.8971 - val_loss: 0.2720 - learning_rate: 9.8907e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9073 - loss: 0.2567 - val_accuracy: 0.8960 - val_loss: 0.2862 - learning_rate: 9.7553e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9155 - loss: 0.2340 - val_accuracy: 0.9036 - val_loss: 0.2706 - learning_rate: 9.5677e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9219 - loss: 0.2166 - val_accuracy: 0.8512 - val_loss: 0.4675 - learning_rate: 9.3301e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9286 - loss: 0.1966 - val_accuracy: 0.9153 - val_loss: 0.2403 - learning_rate: 9.0451e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9332 - loss: 0.1838 - val_accuracy: 0.9080 - val_loss: 0.2749 - learning_rate: 8.7157e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9402 - loss: 0.1652 - val_accuracy: 0.9122 - val_loss: 0.2552 - learning_rate: 8.3457e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9418 - loss: 0.1594 - val_accuracy: 0.9240 - val_loss: 0.2253 - learning_rate: 7.9389e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9497 - loss: 0.1429 - val_accuracy: 0.9245 - val_loss: 0.2277 - learning_rate: 7.5000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9548 - loss: 0.1275 - val_accuracy: 0.8940 - val_loss: 0.3391 - learning_rate: 7.0337e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9583 - loss: 0.1149 - val_accuracy: 0.9197 - val_loss: 0.2516 - learning_rate: 6.5451e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9658 - loss: 0.0969 - val_accuracy: 0.9264 - val_loss: 0.2377 - learning_rate: 6.0396e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9686 - loss: 0.0836 - val_accuracy: 0.9116 - val_loss: 0.2853 - learning_rate: 5.5226e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9752 - loss: 0.0720 - val_accuracy: 0.9254 - val_loss: 0.2604 - learning_rate: 5.0000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9781 - loss: 0.0623 - val_accuracy: 0.9233 - val_loss: 0.2813 - learning_rate: 4.4774e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9825 - loss: 0.0491 - val_accuracy: 0.9188 - val_loss: 0.3124 - learning_rate: 3.9604e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9862 - loss: 0.0402 - val_accuracy: 0.9274 - val_loss: 0.2945 - learning_rate: 3.4549e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9880 - loss: 0.0359 - val_accuracy: 0.9212 - val_loss: 0.3175 - learning_rate: 2.9663e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9907 - loss: 0.0263 - val_accuracy: 0.9282 - val_loss: 0.3006 - learning_rate: 2.5000e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9930 - loss: 0.0209 - val_accuracy: 0.9279 - val_loss: 0.3194 - learning_rate: 2.0611e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9947 - loss: 0.0161 - val_accuracy: 0.9296 - val_loss: 0.3336 - learning_rate: 1.6543e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9957 - loss: 0.0141 - val_accuracy: 0.9291 - val_loss: 0.3494 - learning_rate: 1.2843e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9969 - loss: 0.0107 - val_accuracy: 0.9275 - val_loss: 0.3559 - learning_rate: 9.5492e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9974 - loss: 0.0089 - val_accuracy: 0.9283 - val_loss: 0.3663 - learning_rate: 6.6987e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9975 - loss: 0.0086 - val_accuracy: 0.9296 - val_loss: 0.3579 - learning_rate: 4.3227e-05\n",
            "Epoch 28/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 0.9294 - val_loss: 0.3583 - learning_rate: 2.4472e-05\n",
            "\n",
            "Training Autoencoder CNN model...\n",
            "Epoch 1/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - classifier_output_accuracy: 0.7574 - classifier_output_loss: 0.6629 - decoder_output_loss: 0.0739 - loss: 0.6629 - val_classifier_output_accuracy: 0.8792 - val_classifier_output_loss: 0.3252 - val_decoder_output_loss: 0.0259 - val_loss: 0.3260\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - classifier_output_accuracy: 0.8884 - classifier_output_loss: 0.3040 - decoder_output_loss: 0.0254 - loss: 0.3040 - val_classifier_output_accuracy: 0.9013 - val_classifier_output_loss: 0.2703 - val_decoder_output_loss: 0.0238 - val_loss: 0.2710\n",
            "Epoch 3/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - classifier_output_accuracy: 0.9106 - classifier_output_loss: 0.2453 - decoder_output_loss: 0.0233 - loss: 0.2454 - val_classifier_output_accuracy: 0.9129 - val_classifier_output_loss: 0.2393 - val_decoder_output_loss: 0.0226 - val_loss: 0.2401\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9220 - classifier_output_loss: 0.2105 - decoder_output_loss: 0.0225 - loss: 0.2106 - val_classifier_output_accuracy: 0.9129 - val_classifier_output_loss: 0.2452 - val_decoder_output_loss: 0.0219 - val_loss: 0.2461\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - classifier_output_accuracy: 0.9319 - classifier_output_loss: 0.1850 - decoder_output_loss: 0.0212 - loss: 0.1850 - val_classifier_output_accuracy: 0.9164 - val_classifier_output_loss: 0.2269 - val_decoder_output_loss: 0.0214 - val_loss: 0.2278\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9413 - classifier_output_loss: 0.1575 - decoder_output_loss: 0.0205 - loss: 0.1575 - val_classifier_output_accuracy: 0.9196 - val_classifier_output_loss: 0.2258 - val_decoder_output_loss: 0.0205 - val_loss: 0.2265\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - classifier_output_accuracy: 0.9484 - classifier_output_loss: 0.1361 - decoder_output_loss: 0.0201 - loss: 0.1361 - val_classifier_output_accuracy: 0.9126 - val_classifier_output_loss: 0.2420 - val_decoder_output_loss: 0.0203 - val_loss: 0.2431\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9572 - classifier_output_loss: 0.1147 - decoder_output_loss: 0.0196 - loss: 0.1147 - val_classifier_output_accuracy: 0.9159 - val_classifier_output_loss: 0.2490 - val_decoder_output_loss: 0.0200 - val_loss: 0.2499\n",
            "Epoch 9/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9641 - classifier_output_loss: 0.0958 - decoder_output_loss: 0.0193 - loss: 0.0958 - val_classifier_output_accuracy: 0.9142 - val_classifier_output_loss: 0.2551 - val_decoder_output_loss: 0.0196 - val_loss: 0.2563\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9705 - classifier_output_loss: 0.0787 - decoder_output_loss: 0.0187 - loss: 0.0787 - val_classifier_output_accuracy: 0.9159 - val_classifier_output_loss: 0.2708 - val_decoder_output_loss: 0.0196 - val_loss: 0.2719\n",
            "Epoch 11/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9761 - classifier_output_loss: 0.0667 - decoder_output_loss: 0.0184 - loss: 0.0668 - val_classifier_output_accuracy: 0.9195 - val_classifier_output_loss: 0.2629 - val_decoder_output_loss: 0.0192 - val_loss: 0.2641\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - classifier_output_accuracy: 0.9782 - classifier_output_loss: 0.0577 - decoder_output_loss: 0.0181 - loss: 0.0578 - val_classifier_output_accuracy: 0.9201 - val_classifier_output_loss: 0.2924 - val_decoder_output_loss: 0.0187 - val_loss: 0.2938\n",
            "Epoch 13/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9830 - classifier_output_loss: 0.0452 - decoder_output_loss: 0.0176 - loss: 0.0452 - val_classifier_output_accuracy: 0.9200 - val_classifier_output_loss: 0.3000 - val_decoder_output_loss: 0.0183 - val_loss: 0.3014\n",
            "Epoch 14/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9850 - classifier_output_loss: 0.0421 - decoder_output_loss: 0.0175 - loss: 0.0421 - val_classifier_output_accuracy: 0.9171 - val_classifier_output_loss: 0.3209 - val_decoder_output_loss: 0.0185 - val_loss: 0.3223\n",
            "Epoch 15/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9876 - classifier_output_loss: 0.0326 - decoder_output_loss: 0.0173 - loss: 0.0326 - val_classifier_output_accuracy: 0.9209 - val_classifier_output_loss: 0.3360 - val_decoder_output_loss: 0.0182 - val_loss: 0.3376\n",
            "Epoch 16/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9875 - classifier_output_loss: 0.0344 - decoder_output_loss: 0.0174 - loss: 0.0344 - val_classifier_output_accuracy: 0.9210 - val_classifier_output_loss: 0.3808 - val_decoder_output_loss: 0.0181 - val_loss: 0.3826\n",
            "Epoch 17/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - classifier_output_accuracy: 0.9907 - classifier_output_loss: 0.0277 - decoder_output_loss: 0.0171 - loss: 0.0277 - val_classifier_output_accuracy: 0.9169 - val_classifier_output_loss: 0.3969 - val_decoder_output_loss: 0.0183 - val_loss: 0.3988\n",
            "Epoch 18/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9875 - classifier_output_loss: 0.0354 - decoder_output_loss: 0.0173 - loss: 0.0355 - val_classifier_output_accuracy: 0.9200 - val_classifier_output_loss: 0.4077 - val_decoder_output_loss: 0.0176 - val_loss: 0.4097\n",
            "Epoch 19/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9899 - classifier_output_loss: 0.0256 - decoder_output_loss: 0.0170 - loss: 0.0257 - val_classifier_output_accuracy: 0.9179 - val_classifier_output_loss: 0.4247 - val_decoder_output_loss: 0.0183 - val_loss: 0.4267\n",
            "Epoch 20/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - classifier_output_accuracy: 0.9920 - classifier_output_loss: 0.0222 - decoder_output_loss: 0.0170 - loss: 0.0222 - val_classifier_output_accuracy: 0.9211 - val_classifier_output_loss: 0.4266 - val_decoder_output_loss: 0.0176 - val_loss: 0.4286\n",
            "Epoch 21/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9914 - classifier_output_loss: 0.0241 - decoder_output_loss: 0.0168 - loss: 0.0241 - val_classifier_output_accuracy: 0.9199 - val_classifier_output_loss: 0.4609 - val_decoder_output_loss: 0.0176 - val_loss: 0.4632\n",
            "Epoch 22/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9925 - classifier_output_loss: 0.0205 - decoder_output_loss: 0.0169 - loss: 0.0205 - val_classifier_output_accuracy: 0.9198 - val_classifier_output_loss: 0.4620 - val_decoder_output_loss: 0.0178 - val_loss: 0.4641\n",
            "Epoch 23/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - classifier_output_accuracy: 0.9875 - classifier_output_loss: 0.0354 - decoder_output_loss: 0.0173 - loss: 0.0354 - val_classifier_output_accuracy: 0.9217 - val_classifier_output_loss: 0.4753 - val_decoder_output_loss: 0.0176 - val_loss: 0.4776\n",
            "Epoch 24/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9950 - classifier_output_loss: 0.0152 - decoder_output_loss: 0.0165 - loss: 0.0152 - val_classifier_output_accuracy: 0.9140 - val_classifier_output_loss: 0.4838 - val_decoder_output_loss: 0.0181 - val_loss: 0.4853\n",
            "Epoch 25/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9933 - classifier_output_loss: 0.0205 - decoder_output_loss: 0.0168 - loss: 0.0205 - val_classifier_output_accuracy: 0.9158 - val_classifier_output_loss: 0.5214 - val_decoder_output_loss: 0.0176 - val_loss: 0.5239\n",
            "Epoch 26/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9942 - classifier_output_loss: 0.0156 - decoder_output_loss: 0.0165 - loss: 0.0156 - val_classifier_output_accuracy: 0.9140 - val_classifier_output_loss: 0.5233 - val_decoder_output_loss: 0.0177 - val_loss: 0.5258\n",
            "Epoch 27/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9924 - classifier_output_loss: 0.0211 - decoder_output_loss: 0.0168 - loss: 0.0211 - val_classifier_output_accuracy: 0.9154 - val_classifier_output_loss: 0.5203 - val_decoder_output_loss: 0.0182 - val_loss: 0.5228\n",
            "Epoch 28/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9917 - classifier_output_loss: 0.0247 - decoder_output_loss: 0.0168 - loss: 0.0247 - val_classifier_output_accuracy: 0.9179 - val_classifier_output_loss: 0.5037 - val_decoder_output_loss: 0.0181 - val_loss: 0.5061\n",
            "Epoch 29/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - classifier_output_accuracy: 0.9971 - classifier_output_loss: 0.0085 - decoder_output_loss: 0.0164 - loss: 0.0085 - val_classifier_output_accuracy: 0.9120 - val_classifier_output_loss: 0.5726 - val_decoder_output_loss: 0.0184 - val_loss: 0.5754\n",
            "Epoch 30/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - classifier_output_accuracy: 0.9944 - classifier_output_loss: 0.0167 - decoder_output_loss: 0.0168 - loss: 0.0167 - val_classifier_output_accuracy: 0.9161 - val_classifier_output_loss: 0.5213 - val_decoder_output_loss: 0.0184 - val_loss: 0.5238\n",
            "\n",
            "Training Capsule Network model...\n",
            "CapsNet model outputs: ['keras_tensor_482', 'keras_tensor_489']\n",
            "CapsNet metrics names: ['loss', 'compile_metrics']\n",
            "Epoch 1/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - capsnet_output_accuracy: 0.6865 - capsnet_output_loss: 0.2340 - decoder_output_loss: 0.1399 - loss: 0.2341\n",
            "Available metrics after first epoch:\n",
            "  - capsnet_output_accuracy\n",
            "  - capsnet_output_loss\n",
            "  - decoder_output_loss\n",
            "  - loss\n",
            "  - val_capsnet_output_accuracy\n",
            "  - val_capsnet_output_loss\n",
            "  - val_decoder_output_loss\n",
            "  - val_loss\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 52ms/step - capsnet_output_accuracy: 0.6867 - capsnet_output_loss: 0.2339 - decoder_output_loss: 0.1399 - loss: 0.2340 - val_capsnet_output_accuracy: 0.8592 - val_capsnet_output_loss: 0.1080 - val_decoder_output_loss: 0.0772 - val_loss: 0.1083\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 44ms/step - capsnet_output_accuracy: 0.8695 - capsnet_output_loss: 0.0987 - decoder_output_loss: 0.0672 - loss: 0.0987 - val_capsnet_output_accuracy: 0.8745 - val_capsnet_output_loss: 0.0949 - val_decoder_output_loss: 0.0553 - val_loss: 0.0951\n",
            "Epoch 3/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - capsnet_output_accuracy: 0.8892 - capsnet_output_loss: 0.0848 - decoder_output_loss: 0.0541 - loss: 0.0849 - val_capsnet_output_accuracy: 0.8843 - val_capsnet_output_loss: 0.0864 - val_decoder_output_loss: 0.0515 - val_loss: 0.0867\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - capsnet_output_accuracy: 0.9004 - capsnet_output_loss: 0.0755 - decoder_output_loss: 0.0510 - loss: 0.0755 - val_capsnet_output_accuracy: 0.8828 - val_capsnet_output_loss: 0.0898 - val_decoder_output_loss: 0.0500 - val_loss: 0.0900\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - capsnet_output_accuracy: 0.9128 - capsnet_output_loss: 0.0680 - decoder_output_loss: 0.0495 - loss: 0.0681 - val_capsnet_output_accuracy: 0.8920 - val_capsnet_output_loss: 0.0809 - val_decoder_output_loss: 0.0486 - val_loss: 0.0811\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - capsnet_output_accuracy: 0.9205 - capsnet_output_loss: 0.0630 - decoder_output_loss: 0.0486 - loss: 0.0630 - val_capsnet_output_accuracy: 0.8871 - val_capsnet_output_loss: 0.0834 - val_decoder_output_loss: 0.0475 - val_loss: 0.0837\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - capsnet_output_accuracy: 0.9268 - capsnet_output_loss: 0.0577 - decoder_output_loss: 0.0479 - loss: 0.0577 - val_capsnet_output_accuracy: 0.9049 - val_capsnet_output_loss: 0.0740 - val_decoder_output_loss: 0.0471 - val_loss: 0.0743\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - capsnet_output_accuracy: 0.9360 - capsnet_output_loss: 0.0514 - decoder_output_loss: 0.0470 - loss: 0.0514 - val_capsnet_output_accuracy: 0.8969 - val_capsnet_output_loss: 0.0784 - val_decoder_output_loss: 0.0466 - val_loss: 0.0787\n",
            "Epoch 9/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - capsnet_output_accuracy: 0.9427 - capsnet_output_loss: 0.0472 - decoder_output_loss: 0.0469 - loss: 0.0473 - val_capsnet_output_accuracy: 0.9022 - val_capsnet_output_loss: 0.0744 - val_decoder_output_loss: 0.0467 - val_loss: 0.0747\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - capsnet_output_accuracy: 0.9471 - capsnet_output_loss: 0.0433 - decoder_output_loss: 0.0464 - loss: 0.0434 - val_capsnet_output_accuracy: 0.8989 - val_capsnet_output_loss: 0.0780 - val_decoder_output_loss: 0.0465 - val_loss: 0.0783\n",
            "Epoch 11/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - capsnet_output_accuracy: 0.9547 - capsnet_output_loss: 0.0392 - decoder_output_loss: 0.0460 - loss: 0.0392 - val_capsnet_output_accuracy: 0.8997 - val_capsnet_output_loss: 0.0753 - val_decoder_output_loss: 0.0459 - val_loss: 0.0755\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - capsnet_output_accuracy: 0.9579 - capsnet_output_loss: 0.0361 - decoder_output_loss: 0.0457 - loss: 0.0361 - val_capsnet_output_accuracy: 0.9000 - val_capsnet_output_loss: 0.0751 - val_decoder_output_loss: 0.0455 - val_loss: 0.0754\n",
            "\n",
            "Final metrics tracked during training:\n",
            "  - capsnet_output_accuracy\n",
            "  - capsnet_output_loss\n",
            "  - decoder_output_loss\n",
            "  - loss\n",
            "  - val_capsnet_output_accuracy\n",
            "  - val_capsnet_output_loss\n",
            "  - val_decoder_output_loss\n",
            "  - val_loss\n",
            "\n",
            "Plotting training history...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-c4adfcd4a309>:282: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
            "  plt.legend()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating all models...\n",
            "\n",
            "Evaluating Attention-Enhanced CNN model...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9251 - loss: 0.3642\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
            "\n",
            "Evaluating Autoencoder CNN model...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "\n",
            "Evaluating Capsule Network model...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step\n",
            "\n",
            "AE-CNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top       0.90      0.85      0.87      1000\n",
            "     Trouser       0.99      0.99      0.99      1000\n",
            "    Pullover       0.90      0.90      0.90      1000\n",
            "       Dress       0.93      0.91      0.92      1000\n",
            "        Coat       0.87      0.91      0.89      1000\n",
            "      Sandal       0.99      0.98      0.99      1000\n",
            "       Shirt       0.77      0.79      0.78      1000\n",
            "     Sneaker       0.95      0.98      0.97      1000\n",
            "         Bag       0.99      0.99      0.99      1000\n",
            "  Ankle boot       0.98      0.95      0.97      1000\n",
            "\n",
            "    accuracy                           0.93     10000\n",
            "   macro avg       0.93      0.93      0.93     10000\n",
            "weighted avg       0.93      0.93      0.93     10000\n",
            "\n",
            "\n",
            "Autoencoder CNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top       0.88      0.87      0.87      1000\n",
            "     Trouser       0.99      0.98      0.99      1000\n",
            "    Pullover       0.85      0.84      0.85      1000\n",
            "       Dress       0.91      0.92      0.92      1000\n",
            "        Coat       0.80      0.90      0.85      1000\n",
            "      Sandal       0.99      0.98      0.99      1000\n",
            "       Shirt       0.80      0.71      0.75      1000\n",
            "     Sneaker       0.96      0.97      0.97      1000\n",
            "         Bag       0.99      0.99      0.99      1000\n",
            "  Ankle boot       0.97      0.96      0.97      1000\n",
            "\n",
            "    accuracy                           0.91     10000\n",
            "   macro avg       0.91      0.91      0.91     10000\n",
            "weighted avg       0.91      0.91      0.91     10000\n",
            "\n",
            "\n",
            "Capsule Network Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top       0.84      0.87      0.86      1000\n",
            "     Trouser       0.99      0.98      0.98      1000\n",
            "    Pullover       0.81      0.83      0.82      1000\n",
            "       Dress       0.92      0.92      0.92      1000\n",
            "        Coat       0.78      0.86      0.82      1000\n",
            "      Sandal       0.99      0.97      0.98      1000\n",
            "       Shirt       0.77      0.66      0.71      1000\n",
            "     Sneaker       0.93      0.98      0.96      1000\n",
            "         Bag       0.98      0.97      0.98      1000\n",
            "  Ankle boot       0.98      0.95      0.96      1000\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n",
            "\n",
            "Overall Performance Metrics:\n",
            "                 Test Accuracy (%)  Precision (Macro) (%)  Recall (Macro) (%)  \\\n",
            "AE-CNN                       92.64              92.705111               92.64   \n",
            "Autoencoder CNN              91.40              91.396073               91.40   \n",
            "Capsule Network              89.92              89.935339               89.92   \n",
            "\n",
            "                 F1 Score (Macro) (%)  \n",
            "AE-CNN                      92.654698  \n",
            "Autoencoder CNN             91.337045  \n",
            "Capsule Network             89.847714  \n",
            "\n",
            "Visualizing reconstructions...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 751ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 758ms/step\n",
            "\n",
            "================================================================================\n",
            "                   MODEL COMPARISON SUMMARY\n",
            "================================================================================\n",
            "                 Test Accuracy (%)  Precision (Macro) (%)  Recall (Macro) (%)  F1 Score (Macro) (%)\n",
            "AE-CNN                       92.64              92.705111               92.64             92.654698\n",
            "Autoencoder CNN              91.40              91.396073               91.40             91.337045\n",
            "Capsule Network              89.92              89.935339               89.92             89.847714\n",
            "\n",
            "Advantages of Attention-Enhanced CNN:\n",
            "1. Highest accuracy and F1 score\n",
            "2. Focuses on relevant spatial features through attention mechanisms\n",
            "3. More efficient training compared to Capsule Network\n",
            "\n",
            "Advantages of Autoencoder CNN:\n",
            "1. Provides meaningful latent space representations\n",
            "2. Can reconstruct input images\n",
            "3. Good balance between performance and complexity\n",
            "\n",
            "Advantages of Capsule Network:\n",
            "1. Preserves spatial relationships between features\n",
            "2. More robust to pose variations (rotations, etc.)\n",
            "3. Requires less training data for good performance\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}