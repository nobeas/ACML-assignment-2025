{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNWEK8fLMoPKDpzu1704OG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nobeas/ACML-assignment-2025/blob/main/Comparison_Capnet_%26_AE_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "dXt9rL8ERmb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "jiiOlK0URr-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading and Processing data**"
      ],
      "metadata": {
        "id": "HkLzfrYCSIn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Reshape images to add channel dimension\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Create validation split\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train, y_train, test_size=10000, random_state=42\n",
        ")\n",
        "\n",
        "# Save original labels before one-hot encoding for metrics calculation\n",
        "y_test_orig = y_test.copy()\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Class names for visualizations\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiavrHeASNaC",
        "outputId": "073bd80b-de73-4992-d800-6d06799c6894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define AE-CNN**"
      ],
      "metadata": {
        "id": "JdMKX1fySSAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Channel Attention Module\n",
        "def channel_attention(x, ratio=16):\n",
        "    channel = x.shape[-1]\n",
        "\n",
        "    # Global average pooling\n",
        "    avg_pool = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # MLP with hidden layer\n",
        "    dense1 = layers.Dense(channel // ratio, activation='relu')(avg_pool)\n",
        "    dense2 = layers.Dense(channel, activation='sigmoid')(dense1)\n",
        "\n",
        "    # Reshape to broadcasting dimensions\n",
        "    dense2 = layers.Reshape((1, 1, channel))(dense2)\n",
        "\n",
        "    # Apply attention\n",
        "    output = layers.Multiply()([x, dense2])\n",
        "\n",
        "    return output\n",
        "\n",
        "# Define Spatial Attention Module\n",
        "def spatial_attention(x, kernel_size=7):\n",
        "    # Average pooling across channels using Keras operations\n",
        "    avg_pool = layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(x)\n",
        "\n",
        "    # Max pooling across channels using Keras operations\n",
        "    max_pool = layers.Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(x)\n",
        "\n",
        "    # Concatenate pooled features\n",
        "    concat = layers.Concatenate()([avg_pool, max_pool])\n",
        "\n",
        "    # Apply convolution to generate attention map\n",
        "    spatial_map = layers.Conv2D(1, kernel_size,\n",
        "                              padding='same',\n",
        "                              activation='sigmoid',\n",
        "                              kernel_initializer='he_normal')(concat)\n",
        "\n",
        "    # Apply attention\n",
        "    output = layers.Multiply()([x, spatial_map])\n",
        "\n",
        "    return output\n",
        "\n",
        "# Build the AE-CNN model\n",
        "def build_ae_cnn_model():\n",
        "    inputs = layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "    # Conv Block 1\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Conv Block 2\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Apply Channel Attention\n",
        "    x = channel_attention(x, ratio=16)\n",
        "\n",
        "    # Apply Spatial Attention\n",
        "    x = spatial_attention(x, kernel_size=7)\n",
        "\n",
        "    # Conv Block 3\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "LC3tBL1tSV1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Capsule Network Model**"
      ],
      "metadata": {
        "id": "MvWbpKHSSbDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CapsuleLayer(layers.Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.W = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_dim = input_shape[-1]\n",
        "        self.W = self.add_weight(\n",
        "            shape=[1, self.input_dim, self.num_capsule, self.dim_capsule, 1],\n",
        "            initializer='glorot_uniform',\n",
        "            name='W')\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Reshape the input\n",
        "        # inputs shape: [batch_size, input_dim]\n",
        "        # We need to reshape it to [batch_size, input_dim, 1, 1]\n",
        "        input_expanded = tf.expand_dims(tf.expand_dims(inputs, -1), -1)\n",
        "\n",
        "        # Prepare the input for matmul with W\n",
        "        # [batch_size, input_dim, 1, 1] -> [batch_size, input_dim, num_capsule, 1, 1]\n",
        "        input_tiled = tf.tile(input_expanded, [1, 1, self.num_capsule, 1, 1])\n",
        "\n",
        "        # Reshape W to be compatible with input_tiled\n",
        "        # [1, input_dim, num_capsule, dim_capsule, 1] -> [batch_size, input_dim, num_capsule, dim_capsule, 1]\n",
        "        W_tiled = tf.tile(self.W, [tf.shape(inputs)[0], 1, 1, 1, 1])\n",
        "\n",
        "        # Now perform a transformation on each primary capsule\n",
        "        # [batch_size, input_dim, num_capsule, dim_capsule, 1] @ [batch_size, input_dim, num_capsule, 1, 1]\n",
        "        # -> [batch_size, input_dim, num_capsule, dim_capsule, 1]\n",
        "        inputs_hat = tf.matmul(W_tiled, input_tiled)\n",
        "\n",
        "        # Remove last dimension\n",
        "        # [batch_size, input_dim, num_capsule, dim_capsule, 1] -> [batch_size, input_dim, num_capsule, dim_capsule]\n",
        "        inputs_hat = tf.squeeze(inputs_hat, -1)\n",
        "\n",
        "        # Routing algorithm\n",
        "        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.input_dim, self.num_capsule, 1])\n",
        "\n",
        "        for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, axis=2)\n",
        "            c = tf.expand_dims(c, -1)\n",
        "            outputs = tf.reduce_sum(c * inputs_hat, axis=1, keepdims=True)\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                outputs = squash(outputs)\n",
        "                b += tf.reduce_sum(tf.matmul(\n",
        "                    tf.expand_dims(inputs_hat, -1),\n",
        "                    tf.expand_dims(outputs, -2)\n",
        "                ), axis=-1)\n",
        "\n",
        "        outputs = squash(outputs)\n",
        "        return tf.reshape(outputs, [-1, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "# Build the Capsule Network model\n",
        "def build_capsule_network():\n",
        "    inputs = layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "    # Conv Block 1\n",
        "    x = layers.Conv2D(256, (9, 9), padding='valid', activation='relu')(inputs)\n",
        "\n",
        "    # Primary Capsule Layer\n",
        "    primary_caps = layers.Conv2D(256, (9, 9), strides=2, padding='valid')(x)\n",
        "    primary_caps = layers.Reshape(target_shape=[-1, 8])(primary_caps)\n",
        "    primary_caps = layers.Lambda(lambda x: tf.sqrt(tf.reduce_sum(tf.square(x), -1) + 1e-7))(primary_caps)\n",
        "\n",
        "    # Digit Capsule Layer\n",
        "    digit_caps = CapsuleLayer(num_capsule=10, dim_capsule=16, routings=3)(primary_caps)\n",
        "\n",
        "    # Output Layer\n",
        "    outputs = layers.Lambda(lambda x: tf.sqrt(tf.reduce_sum(tf.square(x), -1) + 1e-7))(digit_caps)\n",
        "\n",
        "    # Masking\n",
        "    y = layers.Input(shape=(10,))\n",
        "    masked = layers.Multiply()([outputs, y])\n",
        "\n",
        "    # Decoder\n",
        "    decoder = models.Sequential([\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1024, activation='relu'),\n",
        "        layers.Dense(784, activation='sigmoid'),\n",
        "        layers.Reshape(target_shape=(28, 28, 1))\n",
        "    ])\n",
        "\n",
        "    # Reconstruct the input\n",
        "    reconstructed = decoder(masked)\n",
        "\n",
        "    # Combine the models\n",
        "    model = models.Model([inputs, y], [outputs, reconstructed])\n",
        "\n",
        "    # Margin loss\n",
        "    def margin_loss(y_true, y_pred):\n",
        "        L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
        "            0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
        "        return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss=[margin_loss, 'mse'],\n",
        "        loss_weights=[1., 0.0005],\n",
        "        metrics={'outputs': 'accuracy'}\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "PjpKddufSgzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train & Evaluate models**"
      ],
      "metadata": {
        "id": "yYGCZkD3SlCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Define model parameters globally for easy access\n",
        "input_shape = 784  # 28*28\n",
        "num_classes = 10\n",
        "\n",
        "# Squash function for Capsule Networks\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule.\n",
        "    \"\"\"\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + 1e-8)\n",
        "    return scale * vectors\n",
        "\n",
        "# Very simple CapsuleLayer using basic TensorFlow operations\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.W = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Input shape: [batch_size, input_num_capsule, input_dim_capsule]\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Define weight matrix [input_num_capsule, num_capsule, input_dim_capsule, dim_capsule]\n",
        "        self.W = self.add_weight(\n",
        "            shape=[self.input_num_capsule, self.num_capsule, self.input_dim_capsule, self.dim_capsule],\n",
        "            initializer='glorot_uniform',\n",
        "            name='capsule_weights')\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: [batch_size, input_num_capsule, input_dim_capsule]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        # inputs_hat: [batch_size, input_num_capsule, num_capsule, dim_capsule]\n",
        "        # Expand and reshape for broadcasting\n",
        "        inputs_expanded = tf.expand_dims(tf.expand_dims(inputs, 2), 4)  # [batch_size, input_num_capsule, 1, input_dim_capsule, 1]\n",
        "        W_expanded = tf.expand_dims(self.W, 0)  # [1, input_num_capsule, num_capsule, input_dim_capsule, dim_capsule]\n",
        "\n",
        "        # Manually compute the matrix multiplication across the batch\n",
        "        # First, we tile the weight matrix for each item in the batch\n",
        "        W_tiled = tf.tile(W_expanded, [batch_size, 1, 1, 1, 1])  # [batch_size, input_num_capsule, num_capsule, input_dim_capsule, dim_capsule]\n",
        "\n",
        "        # Then, we tile the input for each output capsule\n",
        "        inputs_tiled = tf.tile(inputs_expanded, [1, 1, self.num_capsule, 1, self.dim_capsule])  # [batch_size, input_num_capsule, num_capsule, input_dim_capsule, dim_capsule]\n",
        "\n",
        "        # Multiply the inputs with the weight matrix\n",
        "        # [batch_size, input_num_capsule, num_capsule, input_dim_capsule, dim_capsule]\n",
        "        u_hat_raw = W_tiled * inputs_tiled\n",
        "\n",
        "        # Sum over the input_dim_capsule dimension\n",
        "        # [batch_size, input_num_capsule, num_capsule, dim_capsule]\n",
        "        u_hat = tf.reduce_sum(u_hat_raw, axis=3)\n",
        "\n",
        "        # Initialize the routing logits\n",
        "        b = tf.zeros([batch_size, self.input_num_capsule, self.num_capsule, 1])\n",
        "\n",
        "        # Routing algorithm\n",
        "        for i in range(self.routings):\n",
        "            # c_ij: [batch_size, input_num_capsule, num_capsule, 1]\n",
        "            c = tf.nn.softmax(b, axis=2)\n",
        "\n",
        "            # Multiply u_hat by c\n",
        "            # [batch_size, input_num_capsule, num_capsule, dim_capsule] * [batch_size, input_num_capsule, num_capsule, 1]\n",
        "            # -> [batch_size, input_num_capsule, num_capsule, dim_capsule]\n",
        "            weighted = c * u_hat\n",
        "\n",
        "            # Sum over input_num_capsule\n",
        "            # [batch_size, input_num_capsule, num_capsule, dim_capsule] -> [batch_size, num_capsule, dim_capsule]\n",
        "            s = tf.reduce_sum(weighted, axis=1)\n",
        "\n",
        "            # Apply squashing\n",
        "            v = squash(s)\n",
        "\n",
        "            # Update routing weights if not the last iteration\n",
        "            if i < self.routings - 1:\n",
        "                # v: [batch_size, num_capsule, dim_capsule]\n",
        "                # u_hat: [batch_size, input_num_capsule, num_capsule, dim_capsule]\n",
        "\n",
        "                # Expand v for broadcasting\n",
        "                v_expanded = tf.expand_dims(v, 1)  # [batch_size, 1, num_capsule, dim_capsule]\n",
        "\n",
        "                # Calculate agreement\n",
        "                # [batch_size, 1, num_capsule, dim_capsule] * [batch_size, input_num_capsule, num_capsule, dim_capsule]\n",
        "                # -> [batch_size, input_num_capsule, num_capsule, dim_capsule]\n",
        "                agreement = tf.reduce_sum(v_expanded * u_hat, -1, keepdims=True)\n",
        "\n",
        "                # Update the routing logits\n",
        "                b = b + agreement\n",
        "\n",
        "        return v\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.num_capsule, self.dim_capsule)\n",
        "\n",
        "# Custom mask function\n",
        "def mask(inputs):\n",
        "    # inputs: [capsule_output, y_true]\n",
        "    # capsule_output: [batch_size, num_capsule, dim_capsule]\n",
        "    # y_true: [batch_size, num_classes]\n",
        "    capsule_output = inputs[0]\n",
        "    y = inputs[1]\n",
        "\n",
        "    # Expand y for broadcasting\n",
        "    # [batch_size, num_classes] -> [batch_size, num_classes, 1]\n",
        "    mask_expanded = tf.expand_dims(y, -1)\n",
        "\n",
        "    # Apply mask: [batch_size, num_classes, dim_capsule]\n",
        "    masked = capsule_output * mask_expanded\n",
        "\n",
        "    # Flatten for decoder: [batch_size, num_classes * dim_capsule]\n",
        "    masked_flattened = tf.reshape(masked, [-1, num_classes * 16])\n",
        "\n",
        "    return masked_flattened\n",
        "\n",
        "# Margin Loss for Capsule Networks\n",
        "class MarginLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, margin=0.9, downweight=0.5, **kwargs):\n",
        "        super(MarginLoss, self).__init__(**kwargs)\n",
        "        self.margin = margin\n",
        "        self.downweight = downweight\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Convert to one-hot if it's not already\n",
        "        y_true_shape = tf.shape(y_true)\n",
        "        if y_true_shape.shape[0] == 1 or (y_true_shape.shape > 0 and y_true_shape[-1] != num_classes):\n",
        "            y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n",
        "\n",
        "        # Calculate L+ and L-\n",
        "        L_plus = y_true * tf.square(tf.maximum(0., self.margin - y_pred))\n",
        "        L_minus = (1 - y_true) * tf.square(tf.maximum(0., y_pred - (1 - self.margin))) * self.downweight\n",
        "\n",
        "        # Sum all losses\n",
        "        return tf.reduce_mean(tf.reduce_sum(L_plus + L_minus, 1))\n",
        "\n",
        "# Function to build the Auto-Encoder CNN model\n",
        "def build_ae_cnn_model():\n",
        "    # Encoder\n",
        "    inputs = layers.Input(shape=(input_shape,))\n",
        "    x = layers.Reshape((28, 28, 1))(inputs)\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 4x4x128\n",
        "\n",
        "    # Latent space\n",
        "    x = layers.Flatten()(x)  # 2048\n",
        "    encoded = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "    # Classification from latent space\n",
        "    classifier = layers.Dense(num_classes, activation='softmax')(encoded)\n",
        "\n",
        "    # Decoder\n",
        "    x = layers.Dense(2048, activation='relu')(encoded)\n",
        "    x = layers.Reshape((4, 4, 128))(x)\n",
        "    x = layers.Conv2DTranspose(128, (3, 3), strides=2, activation='relu', padding='same')(x)  # 8x8x128\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(x)   # 16x16x64\n",
        "    x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same')(x)   # 32x32x32\n",
        "    x = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)                    # 32x32x1\n",
        "    x = layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)  # 28x28x1\n",
        "    decoded = layers.Flatten()(x)  # Back to 784\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=inputs, outputs=classifier)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to build the Capsule Network model\n",
        "def build_capsule_network():\n",
        "    # Input layers\n",
        "    x_input = layers.Input(shape=(input_shape,))\n",
        "    y_input = layers.Input(shape=(num_classes,))\n",
        "\n",
        "    # Reshape inputs to work with Conv2D layers\n",
        "    x_reshaped = layers.Reshape((28, 28, 1))(x_input)\n",
        "\n",
        "    # Primary Capsule layer (Conv2D + reshape)\n",
        "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu')(x_reshaped)\n",
        "    # Primary caps - converting from conv2d to capsule shape\n",
        "    primarycaps = layers.Conv2D(filters=32*8, kernel_size=9, strides=2, padding='valid')(conv1)\n",
        "\n",
        "    # Reshape to [batch_size, num_capsules, dim_capsule]\n",
        "    primarycaps_reshaped = layers.Reshape((-1, 8))(primarycaps)  # 1152 capsules with 8-dim each\n",
        "\n",
        "    # Squash the capsules\n",
        "    primarycaps_squashed = layers.Lambda(lambda x: squash(x))(primarycaps_reshaped)\n",
        "\n",
        "    # DigitCaps layer (CapsuleLayer)\n",
        "    digitcaps = CapsuleLayer(num_capsule=num_classes, dim_capsule=16, routings=3)(primarycaps_squashed)\n",
        "\n",
        "    # Length layer - for classification output\n",
        "    out_caps = layers.Lambda(lambda x: tf.sqrt(tf.reduce_sum(tf.square(x), -1)))(digitcaps)\n",
        "\n",
        "    # Mask the capsule outputs for reconstruction\n",
        "    masked = layers.Lambda(lambda x: mask(x))([digitcaps, y_input])\n",
        "\n",
        "    # Decoder network\n",
        "    decoder = layers.Dense(512, activation='relu')(masked)\n",
        "    decoder = layers.Dense(1024, activation='relu')(decoder)\n",
        "    decoder = layers.Dense(input_shape, activation='sigmoid')(decoder)\n",
        "\n",
        "    # Models for training and evaluation\n",
        "    model = Model([x_input, y_input], [out_caps, decoder])\n",
        "\n",
        "    # Compile the model with categorical crossentropy instead of margin loss for simplicity\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss=['sparse_categorical_crossentropy', 'mse'],\n",
        "        loss_weights=[1.0, 0.0005],\n",
        "        metrics=[['accuracy'], ['mse']]  # Specify metrics for each output\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train_orig), (x_test, y_test_orig) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize and reshape data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train_orig, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test_orig, 10)\n",
        "\n",
        "# Split into training and validation sets\n",
        "val_size = 10000\n",
        "x_val = x_train[-val_size:]\n",
        "y_val = y_train[-val_size:]\n",
        "y_val_orig = y_train_orig[-val_size:]\n",
        "x_train = x_train[:-val_size]\n",
        "y_train = y_train[:-val_size]\n",
        "y_train_orig = y_train_orig[:-val_size]\n",
        "\n",
        "# Build the AE-CNN model\n",
        "ae_cnn_model = build_ae_cnn_model()\n",
        "\n",
        "# Build the Capsule Network model\n",
        "capsule_model = build_capsule_network()\n",
        "\n",
        "# Define callback for early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the AE-CNN model\n",
        "print(\"Training the AE-CNN model...\")\n",
        "ae_cnn_history = ae_cnn_model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the Capsule Network model\n",
        "print(\"Training the Capsule Network model...\")\n",
        "capsule_history = capsule_model.fit(\n",
        "    [x_train, y_train],  # inputs: x_train and one-hot encoded labels\n",
        "    [y_train_orig, x_train],  # targets: integer labels for sparse_categorical_crossentropy, original images for reconstruction\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        "    validation_data=([x_val, y_val], [y_val_orig, x_val]),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate AE-CNN model on test set\n",
        "print(\"\\nEvaluating AE-CNN model on test set...\")\n",
        "ae_cnn_test_loss, ae_cnn_test_acc = ae_cnn_model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f\"AE-CNN Test accuracy: {ae_cnn_test_acc:.4f}\")\n",
        "\n",
        "# Evaluate Capsule Network model on test set\n",
        "print(\"\\nEvaluating Capsule Network model on test set...\")\n",
        "capsule_test_results = capsule_model.evaluate(\n",
        "    [x_test, y_test],\n",
        "    [y_test_orig, x_test],\n",
        "    verbose=1\n",
        ")\n",
        "capsule_test_loss = capsule_test_results[0]  # Total loss\n",
        "capsule_test_acc = capsule_test_results[3]   # Accuracy (metric for first output)\n",
        "print(f\"Capsule Network Test accuracy: {capsule_test_acc:.4f}\")\n",
        "\n",
        "# Get predictions\n",
        "print(\"Generating predictions...\")\n",
        "ae_cnn_y_pred_prob = ae_cnn_model.predict(x_test)\n",
        "ae_cnn_y_pred = np.argmax(ae_cnn_y_pred_prob, axis=1)\n",
        "\n",
        "capsule_predictions = capsule_model.predict([x_test, y_test])\n",
        "capsule_y_pred_prob = capsule_predictions[0]  # First output is class probabilities\n",
        "capsule_y_pred = np.argmax(capsule_y_pred_prob, axis=1)\n",
        "\n",
        "# Calculate performance metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision_macro = precision_score(y_true, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_true, y_pred, average='macro')\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    return accuracy, precision_macro, recall_macro, f1_macro\n",
        "\n",
        "ae_cnn_metrics = calculate_metrics(y_test_orig, ae_cnn_y_pred)\n",
        "capsule_metrics = calculate_metrics(y_test_orig, capsule_y_pred)\n",
        "\n",
        "# Store metrics in a dictionary\n",
        "metrics = {\n",
        "    'AE-CNN': {\n",
        "        'Test Accuracy': ae_cnn_metrics[0],\n",
        "        'Precision (Macro)': ae_cnn_metrics[1],\n",
        "        'Recall (Macro)': ae_cnn_metrics[2],\n",
        "        'F1 Score (Macro)': ae_cnn_metrics[3]\n",
        "    },\n",
        "    'Capsule Network': {\n",
        "        'Test Accuracy': capsule_metrics[0],\n",
        "        'Precision (Macro)': capsule_metrics[1],\n",
        "        'Recall (Macro)': capsule_metrics[2],\n",
        "        'F1 Score (Macro)': capsule_metrics[3]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display metrics as a DataFrame\n",
        "metrics_df = pd.DataFrame(metrics).T * 100\n",
        "metrics_df.columns = ['Test Accuracy (%)', 'Precision (Macro) (%)', 'Recall (Macro) (%)', 'F1 Score (Macro) (%)']\n",
        "print(\"\\nOverall Performance Metrics:\")\n",
        "print(metrics_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnxrUJ6Hg7la",
        "outputId": "96988d80-0b6f-4582-f7aa-24c327a43537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the AE-CNN model...\n",
            "Epoch 1/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8825 - loss: 0.3632 - val_accuracy: 0.9870 - val_loss: 0.0467\n",
            "Epoch 2/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9870 - loss: 0.0428 - val_accuracy: 0.9889 - val_loss: 0.0363\n",
            "Epoch 3/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9917 - loss: 0.0266 - val_accuracy: 0.9907 - val_loss: 0.0329\n",
            "Epoch 4/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9934 - loss: 0.0206 - val_accuracy: 0.9894 - val_loss: 0.0375\n",
            "Epoch 5/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9942 - loss: 0.0175 - val_accuracy: 0.9913 - val_loss: 0.0279\n",
            "Epoch 6/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9957 - loss: 0.0137 - val_accuracy: 0.9904 - val_loss: 0.0332\n",
            "Epoch 7/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9955 - loss: 0.0122 - val_accuracy: 0.9896 - val_loss: 0.0393\n",
            "Epoch 8/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9968 - loss: 0.0095 - val_accuracy: 0.9878 - val_loss: 0.0406\n",
            "Epoch 9/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9970 - loss: 0.0088 - val_accuracy: 0.9918 - val_loss: 0.0340\n",
            "Epoch 10/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9972 - loss: 0.0080 - val_accuracy: 0.9911 - val_loss: 0.0337\n",
            "Training the Capsule Network model...\n",
            "Epoch 1/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 57ms/step - dense_71_loss: 0.1638 - dense_71_mse: 0.1638 - lambda_33_accuracy: 0.6492 - lambda_33_loss: 1.1498 - loss: 1.1499 - val_dense_71_loss: 0.0658 - val_dense_71_mse: 0.0658 - val_lambda_33_accuracy: 0.9796 - val_lambda_33_loss: 0.1457 - val_loss: 0.1461\n",
            "Epoch 2/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - dense_71_loss: 0.0663 - dense_71_mse: 0.0663 - lambda_33_accuracy: 0.9716 - lambda_33_loss: 0.1718 - loss: 0.1719 - val_dense_71_loss: 0.0638 - val_dense_71_mse: 0.0638 - val_lambda_33_accuracy: 0.9824 - val_lambda_33_loss: 0.1125 - val_loss: 0.1129\n",
            "Epoch 3/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - dense_71_loss: 0.0636 - dense_71_mse: 0.0636 - lambda_33_accuracy: 0.9853 - lambda_33_loss: 0.0922 - loss: 0.0922 - val_dense_71_loss: 0.0574 - val_dense_71_mse: 0.0574 - val_lambda_33_accuracy: 0.9863 - val_lambda_33_loss: 0.0799 - val_loss: 0.0801\n",
            "Epoch 4/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - dense_71_loss: 0.0583 - dense_71_mse: 0.0583 - lambda_33_accuracy: 0.9910 - lambda_33_loss: 0.0642 - loss: 0.0642 - val_dense_71_loss: 0.0543 - val_dense_71_mse: 0.0543 - val_lambda_33_accuracy: 0.9855 - val_lambda_33_loss: 0.0824 - val_loss: 0.0827\n",
            "Epoch 5/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - dense_71_loss: 0.0557 - dense_71_mse: 0.0557 - lambda_33_accuracy: 0.9928 - lambda_33_loss: 0.0541 - loss: 0.0542 - val_dense_71_loss: 0.0531 - val_dense_71_mse: 0.0531 - val_lambda_33_accuracy: 0.9866 - val_lambda_33_loss: 0.0675 - val_loss: 0.0678\n",
            "Epoch 6/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - dense_71_loss: 0.0548 - dense_71_mse: 0.0548 - lambda_33_accuracy: 0.9958 - lambda_33_loss: 0.0373 - loss: 0.0374 - val_dense_71_loss: 0.0525 - val_dense_71_mse: 0.0524 - val_lambda_33_accuracy: 0.9854 - val_lambda_33_loss: 0.0774 - val_loss: 0.0777\n",
            "Epoch 7/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - dense_71_loss: 0.0542 - dense_71_mse: 0.0542 - lambda_33_accuracy: 0.9967 - lambda_33_loss: 0.0353 - loss: 0.0353 - val_dense_71_loss: 0.0523 - val_dense_71_mse: 0.0523 - val_lambda_33_accuracy: 0.9855 - val_lambda_33_loss: 0.0676 - val_loss: 0.0679\n",
            "Epoch 8/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - dense_71_loss: 0.0540 - dense_71_mse: 0.0540 - lambda_33_accuracy: 0.9985 - lambda_33_loss: 0.0265 - loss: 0.0265 - val_dense_71_loss: 0.0521 - val_dense_71_mse: 0.0521 - val_lambda_33_accuracy: 0.9858 - val_lambda_33_loss: 0.0644 - val_loss: 0.0647\n",
            "Epoch 9/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - dense_71_loss: 0.0537 - dense_71_mse: 0.0537 - lambda_33_accuracy: 0.9990 - lambda_33_loss: 0.0206 - loss: 0.0206 - val_dense_71_loss: 0.0519 - val_dense_71_mse: 0.0519 - val_lambda_33_accuracy: 0.9850 - val_lambda_33_loss: 0.0629 - val_loss: 0.0631\n",
            "Epoch 10/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - dense_71_loss: 0.0535 - dense_71_mse: 0.0535 - lambda_33_accuracy: 0.9990 - lambda_33_loss: 0.0191 - loss: 0.0191 - val_dense_71_loss: 0.0518 - val_dense_71_mse: 0.0518 - val_lambda_33_accuracy: 0.9866 - val_lambda_33_loss: 0.0579 - val_loss: 0.0581\n",
            "Epoch 11/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - dense_71_loss: 0.0534 - dense_71_mse: 0.0534 - lambda_33_accuracy: 0.9993 - lambda_33_loss: 0.0168 - loss: 0.0168 - val_dense_71_loss: 0.0519 - val_dense_71_mse: 0.0519 - val_lambda_33_accuracy: 0.9842 - val_lambda_33_loss: 0.0659 - val_loss: 0.0662\n",
            "Epoch 12/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - dense_71_loss: 0.0534 - dense_71_mse: 0.0534 - lambda_33_accuracy: 0.9992 - lambda_33_loss: 0.0173 - loss: 0.0174 - val_dense_71_loss: 0.0518 - val_dense_71_mse: 0.0518 - val_lambda_33_accuracy: 0.9812 - val_lambda_33_loss: 0.0815 - val_loss: 0.0818\n",
            "Epoch 13/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - dense_71_loss: 0.0534 - dense_71_mse: 0.0534 - lambda_33_accuracy: 0.9988 - lambda_33_loss: 0.0211 - loss: 0.0211 - val_dense_71_loss: 0.0520 - val_dense_71_mse: 0.0520 - val_lambda_33_accuracy: 0.9851 - val_lambda_33_loss: 0.0748 - val_loss: 0.0751\n",
            "Epoch 14/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - dense_71_loss: 0.0535 - dense_71_mse: 0.0535 - lambda_33_accuracy: 0.9995 - lambda_33_loss: 0.0169 - loss: 0.0169 - val_dense_71_loss: 0.0519 - val_dense_71_mse: 0.0519 - val_lambda_33_accuracy: 0.9851 - val_lambda_33_loss: 0.0627 - val_loss: 0.0630\n",
            "Epoch 15/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - dense_71_loss: 0.0532 - dense_71_mse: 0.0532 - lambda_33_accuracy: 0.9997 - lambda_33_loss: 0.0121 - loss: 0.0121 - val_dense_71_loss: 0.0518 - val_dense_71_mse: 0.0518 - val_lambda_33_accuracy: 0.9836 - val_lambda_33_loss: 0.0663 - val_loss: 0.0665\n",
            "\n",
            "Evaluating AE-CNN model on test set...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0306\n",
            "AE-CNN Test accuracy: 0.9927\n",
            "\n",
            "Evaluating Capsule Network model on test set...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - dense_71_loss: 0.0523 - dense_71_mse: 0.0523 - lambda_33_accuracy: 0.9809 - lambda_33_loss: 0.0728 - loss: 0.0728\n",
            "Capsule Network Test accuracy: 0.0529\n",
            "Generating predictions...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "\n",
            "Overall Performance Metrics:\n",
            "                 Test Accuracy (%)  Precision (Macro) (%)  Recall (Macro) (%)  \\\n",
            "AE-CNN                       99.27              99.270603           99.256337   \n",
            "Capsule Network              98.46              98.458719           98.455006   \n",
            "\n",
            "                 F1 Score (Macro) (%)  \n",
            "AE-CNN                      99.262194  \n",
            "Capsule Network             98.454160  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Another code for comparaison**"
      ],
      "metadata": {
        "id": "2Nf-VMFEjvej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Define model parameters globally for easy access\n",
        "input_shape = 784  # 28*28\n",
        "num_classes = 10\n",
        "\n",
        "# Squash function for Capsule Networks\n",
        "def squash(vectors, axis=-1):\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + 1e-8)\n",
        "    return scale * vectors\n",
        "\n",
        "# CapsuleLayer\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.W = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "        self.W = self.add_weight(\n",
        "            shape=[self.input_num_capsule, self.num_capsule, self.input_dim_capsule, self.dim_capsule],\n",
        "            initializer='glorot_uniform',\n",
        "            name='capsule_weights')\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        inputs_expanded = tf.expand_dims(tf.expand_dims(inputs, 2), 4)\n",
        "        W_expanded = tf.expand_dims(self.W, 0)\n",
        "        W_tiled = tf.tile(W_expanded, [batch_size, 1, 1, 1, 1])\n",
        "        inputs_tiled = tf.tile(inputs_expanded, [1, 1, self.num_capsule, 1, self.dim_capsule])\n",
        "        u_hat_raw = W_tiled * inputs_tiled\n",
        "        u_hat = tf.reduce_sum(u_hat_raw, axis=3)\n",
        "        b = tf.zeros([batch_size, self.input_num_capsule, self.num_capsule, 1])\n",
        "        for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, axis=2)\n",
        "            weighted = c * u_hat\n",
        "            s = tf.reduce_sum(weighted, axis=1)\n",
        "            v = squash(s)\n",
        "            if i < self.routings - 1:\n",
        "                v_expanded = tf.expand_dims(v, 1)\n",
        "                agreement = tf.reduce_sum(v_expanded * u_hat, -1, keepdims=True)\n",
        "                b = b + agreement\n",
        "        return v\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.num_capsule, self.dim_capsule)\n",
        "\n",
        "# Custom mask function\n",
        "def mask(inputs):\n",
        "    capsule_output = inputs[0]\n",
        "    y = inputs[1]\n",
        "    mask_expanded = tf.expand_dims(y, -1)\n",
        "    masked = capsule_output * mask_expanded\n",
        "    masked_flattened = tf.reshape(masked, [-1, num_classes * 16])\n",
        "    return masked_flattened\n",
        "\n",
        "# Function to build the Auto-Encoder CNN model\n",
        "def build_ae_cnn_model():\n",
        "    inputs = layers.Input(shape=(input_shape,))\n",
        "    x = layers.Reshape((28, 28, 1))(inputs)\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    encoded = layers.Dense(256, activation='relu')(x)\n",
        "    classifier = layers.Dense(num_classes, activation='softmax')(encoded)\n",
        "    x = layers.Dense(2048, activation='relu')(encoded)\n",
        "    x = layers.Reshape((4, 4, 128))(x)\n",
        "    x = layers.Conv2DTranspose(128, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
        "    x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
        "    x = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "    x = layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
        "    decoded = layers.Flatten()(x)\n",
        "    model = Model(inputs=inputs, outputs=classifier)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Function to build the Capsule Network model\n",
        "def build_capsule_network():\n",
        "    x_input = layers.Input(shape=(input_shape,))\n",
        "    y_input = layers.Input(shape=(num_classes,))\n",
        "    x_reshaped = layers.Reshape((28, 28, 1))(x_input)\n",
        "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu')(x_reshaped)\n",
        "    primarycaps = layers.Conv2D(filters=32*8, kernel_size=9, strides=2, padding='valid')(conv1)\n",
        "    primarycaps_reshaped = layers.Reshape((-1, 8))(primarycaps)\n",
        "    primarycaps_squashed = layers.Lambda(lambda x: squash(x))(primarycaps_reshaped)\n",
        "    digitcaps = CapsuleLayer(num_capsule=num_classes, dim_capsule=16, routings=3, name='digitcaps')(primarycaps_squashed)\n",
        "    out_caps = layers.Lambda(lambda x: tf.sqrt(tf.reduce_sum(tf.square(x), -1)))(digitcaps)\n",
        "    masked = layers.Lambda(lambda x: mask(x))([digitcaps, y_input])\n",
        "    decoder = layers.Dense(512, activation='relu')(masked)\n",
        "    decoder = layers.Dense(1024, activation='relu')(decoder)\n",
        "    decoder = layers.Dense(input_shape, activation='sigmoid')(decoder)\n",
        "    model = Model([x_input, y_input], [out_caps, decoder])\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss=['sparse_categorical_crossentropy', 'mse'],\n",
        "        loss_weights=[1.0, 0.0005],\n",
        "        metrics=[['accuracy'], ['mse']]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess Fashion MNIST data\n",
        "    print(\"Loading Fashion MNIST dataset...\")\n",
        "    (x_train, y_train_orig), (x_test, y_test_orig) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "    print(f\"Dataset shapes: x_train: {x_train.shape}, y_train: {y_train_orig.shape}, x_test: {x_test.shape}, y_test: {y_test_orig.shape}\")\n",
        "    x_train = x_train.astype('float32') / 255.\n",
        "    x_test = x_test.astype('float32') / 255.\n",
        "    x_train = x_train.reshape(-1, 784)\n",
        "    x_test = x_test.reshape(-1, 784)\n",
        "    y_train = tf.keras.utils.to_categorical(y_train_orig, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test_orig, 10)\n",
        "    val_size = 10000\n",
        "    x_val = x_train[-val_size:]\n",
        "    y_val = y_train[-val_size:]\n",
        "    y_val_orig = y_train_orig[-val_size:]\n",
        "    x_train = x_train[:-val_size]\n",
        "    y_train = y_train[:-val_size]\n",
        "    y_train_orig = y_train_orig[:-val_size]\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(25):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(x_train[i].reshape(28, 28), cmap=plt.cm.binary)\n",
        "        plt.xlabel(class_names[y_train_orig[i]])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('fashion_mnist_examples.png')\n",
        "    plt.close()\n",
        "    print(\"Building AE-CNN model...\")\n",
        "    ae_cnn_model = build_ae_cnn_model()\n",
        "    ae_cnn_model.summary()\n",
        "    print(\"Building Capsule Network model...\")\n",
        "    capsule_model = build_capsule_network()\n",
        "    capsule_model.summary()\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "    print(\"Training the AE-CNN model...\")\n",
        "    ae_cnn_history = ae_cnn_model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=64,\n",
        "        epochs=20,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"Training the Capsule Network model...\")\n",
        "    capsule_history = capsule_model.fit(\n",
        "        [x_train, y_train],\n",
        "        [y_train_orig, x_train],\n",
        "        batch_size=64,\n",
        "        epochs=20,\n",
        "        validation_data=([x_val, y_val], [y_val_orig, x_val]),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "    def plot_training_history(ae_history, caps_history):\n",
        "        # Add debug print to see available keys\n",
        "        print(\"AE-CNN history keys:\", list(ae_history.history.keys()))\n",
        "        print(\"CapsNet history keys:\", list(caps_history.history.keys()))\n",
        "\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(ae_history.history['accuracy'], label='AE-CNN Training')\n",
        "        plt.plot(ae_history.history['val_accuracy'], label='AE-CNN Validation')\n",
        "\n",
        "        # Find the correct keys for CapsNet accuracy metrics\n",
        "        # Option 1: Direct accuracy key (most likely case)\n",
        "        if 'accuracy' in caps_history.history:\n",
        "            plt.plot(caps_history.history['accuracy'], label='CapsNet Training')\n",
        "            plt.plot(caps_history.history['val_accuracy'], label='CapsNet Validation')\n",
        "        # Option 2: Output-specific accuracy key\n",
        "        elif 'output_1_accuracy' in caps_history.history:\n",
        "            plt.plot(caps_history.history['output_1_accuracy'], label='CapsNet Training')\n",
        "            plt.plot(caps_history.history['val_output_1_accuracy'], label='CapsNet Validation')\n",
        "        # Option 3: Find any key with 'accuracy' in it\n",
        "        else:\n",
        "            # Find the training accuracy key (any key with 'accuracy' but not 'val')\n",
        "            train_acc_keys = [k for k in caps_history.history.keys()\n",
        "                              if 'accuracy' in k and 'val' not in k]\n",
        "            val_acc_keys = [k for k in caps_history.history.keys()\n",
        "                            if 'accuracy' in k and 'val' in k]\n",
        "\n",
        "            if train_acc_keys and val_acc_keys:\n",
        "                plt.plot(caps_history.history[train_acc_keys[0]], label='CapsNet Training')\n",
        "                plt.plot(caps_history.history[val_acc_keys[0]], label='CapsNet Validation')\n",
        "            else:\n",
        "                print(\"WARNING: Could not find accuracy metrics for CapsNet\")\n",
        "                # Skip plotting accuracy for CapsNet if we can't find appropriate keys\n",
        "\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(ae_history.history['loss'], label='AE-CNN Training')\n",
        "        plt.plot(ae_history.history['val_loss'], label='AE-CNN Validation')\n",
        "        plt.plot(caps_history.history['loss'], label='CapsNet Training')\n",
        "        plt.plot(caps_history.history['val_loss'], label='CapsNet Validation')\n",
        "        plt.title('Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_history.png')\n",
        "        plt.close()\n",
        "\n",
        "    plot_training_history(ae_cnn_history, capsule_history)\n",
        "    print(\"\\nEvaluating AE-CNN model on test set...\")\n",
        "    ae_cnn_test_loss, ae_cnn_test_acc = ae_cnn_model.evaluate(x_test, y_test, verbose=1)\n",
        "    print(f\"AE-CNN Test accuracy: {ae_cnn_test_acc:.4f}\")\n",
        "    print(\"\\nEvaluating Capsule Network model on test set...\")\n",
        "    capsule_test_results = capsule_model.evaluate(\n",
        "        [x_test, y_test],\n",
        "        [y_test_orig, x_test],\n",
        "        verbose=1\n",
        "    )\n",
        "    capsule_test_loss = capsule_test_results[0]\n",
        "    capsule_test_acc = capsule_test_results[3]\n",
        "    print(f\"Capsule Network Test accuracy: {capsule_test_acc:.4f}\")\n",
        "    print(\"Generating predictions...\")\n",
        "    ae_cnn_y_pred_prob = ae_cnn_model.predict(x_test)\n",
        "    ae_cnn_y_pred = np.argmax(ae_cnn_y_pred_prob, axis=1)\n",
        "    capsule_predictions = capsule_model.predict([x_test, y_test])\n",
        "    capsule_y_pred_prob = capsule_predictions[0]\n",
        "    capsule_y_pred = np.argmax(capsule_y_pred_prob, axis=1)\n",
        "    def calculate_metrics(y_true, y_pred):\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision_macro = precision_score(y_true, y_pred, average='macro')\n",
        "        recall_macro = recall_score(y_true, y_pred, average='macro')\n",
        "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "        return accuracy, precision_macro, recall_macro, f1_macro\n",
        "    ae_cnn_metrics = calculate_metrics(y_test_orig, ae_cnn_y_pred)\n",
        "    capsule_metrics = calculate_metrics(y_test_orig, capsule_y_pred)\n",
        "    metrics = {\n",
        "        'AE-CNN': {\n",
        "            'Test Accuracy': ae_cnn_metrics[0],\n",
        "            'Precision (Macro)': ae_cnn_metrics[1],\n",
        "            'Recall (Macro)': ae_cnn_metrics[2],\n",
        "            'F1 Score (Macro)': ae_cnn_metrics[3]\n",
        "        },\n",
        "        'Capsule Network': {\n",
        "            'Test Accuracy': capsule_metrics[0],\n",
        "            'Precision (Macro)': capsule_metrics[1],\n",
        "            'Recall (Macro)': capsule_metrics[2],\n",
        "            'F1 Score (Macro)': capsule_metrics[3]\n",
        "        }\n",
        "    }\n",
        "    metrics_df = pd.DataFrame(metrics).T * 100\n",
        "    metrics_df.columns = ['Test Accuracy (%)', 'Precision (Macro) (%)', 'Recall (Macro) (%)', 'F1 Score (Macro) (%)']\n",
        "    print(\"\\nOverall Performance Metrics:\")\n",
        "    print(metrics_df)\n",
        "    metrics_df.to_csv('model_metrics.csv')\n",
        "    print(\"\\nAE-CNN Classification Report:\")\n",
        "    print(classification_report(y_test_orig, ae_cnn_y_pred, target_names=class_names))\n",
        "    print(\"\\nCapsule Network Classification Report:\")\n",
        "    print(classification_report(y_test_orig, capsule_y_pred, target_names=class_names))\n",
        "    def plot_confusion_matrix(y_true, y_pred, class_names, title, filename):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title(title)\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "    plot_confusion_matrix(y_test_orig, ae_cnn_y_pred, class_names, 'AE-CNN Confusion Matrix', 'ae_cnn_confusion_matrix.png')\n",
        "    plot_confusion_matrix(y_test_orig, capsule_y_pred, class_names, 'Capsule Network Confusion Matrix', 'capsule_confusion_matrix.png')\n",
        "    def plot_predictions(x_test, y_test, y_pred, class_names, title, filename):\n",
        "        correct = np.where(y_test == y_pred)[0]\n",
        "        incorrect = np.where(y_test != y_pred)[0]\n",
        "        correct_sample = correct[:5] if len(correct) >= 5 else correct\n",
        "        incorrect_sample = incorrect[:5] if len(incorrect) >= 5 else incorrect\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        for i, idx in enumerate(correct_sample):\n",
        "            plt.subplot(2, 5, i+1)\n",
        "            plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
        "            plt.title(f\"True: {class_names[y_test[idx]]}\\nPred: {class_names[y_pred[idx]]}\")\n",
        "            plt.axis('off')\n",
        "        for i, idx in enumerate(incorrect_sample):\n",
        "            plt.subplot(2, 5, i+6)\n",
        "            plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
        "            plt.title(f\"True: {class_names[y_test[idx]]}\\nPred: {class_names[y_pred[idx]]}\")\n",
        "            plt.axis('off')\n",
        "        plt.suptitle(title)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "    plot_predictions(x_test, y_test_orig, ae_cnn_y_pred, class_names, 'AE-CNN Predictions', 'ae_cnn_predictions.png')\n",
        "    plot_predictions(x_test, y_test_orig, capsule_y_pred, class_names, 'Capsule Network Predictions', 'capsule_predictions.png')\n",
        "    def plot_reconstructions(model, x_test, y_test, y_test_orig, class_names):\n",
        "        [_, reconstructions] = model.predict([x_test[:10], y_test[:10]])\n",
        "        plt.figure(figsize=(20, 4))\n",
        "        for i in range(10):\n",
        "            ax = plt.subplot(2, 10, i + 1)\n",
        "            plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
        "            plt.title(f\"{class_names[y_test_orig[i]]}\")\n",
        "            plt.gray()\n",
        "            ax.set_axis_off()\n",
        "            ax = plt.subplot(2, 10, i + 11)\n",
        "            plt.imshow(reconstructions[i].reshape(28, 28), cmap='gray')\n",
        "            plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dyQ5Dr5WPNMR",
        "outputId": "607913fb-3217-45d2-8a93-7596338b73d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Fashion MNIST dataset...\n",
            "Dataset shapes: x_train: (60000, 28, 28), y_train: (60000,), x_test: (10000, 28, 28), y_test: (10000,)\n",
            "Building AE-CNN model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_21 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape_15 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_44 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_19 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_45 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_20 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_46 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_21 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m619,786\u001b[0m (2.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">619,786</span> (2.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m619,786\u001b[0m (2.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">619,786</span> (2.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Capsule Network model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_22      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ input_layer_22[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_48 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m,    │     \u001b[38;5;34m20,992\u001b[0m │ reshape_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_49 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m5,308,672\u001b[0m │ conv2d_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv2d_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_17 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ reshape_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ digitcaps           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │  \u001b[38;5;34m1,474,560\u001b[0m │ lambda_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mCapsuleLayer\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_23      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_19 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ digitcaps[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ input_layer_23[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m82,432\u001b[0m │ lambda_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │    \u001b[38;5;34m525,312\u001b[0m │ dense_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_18 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ digitcaps[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)       │    \u001b[38;5;34m803,600\u001b[0m │ dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_22      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,992</span> │ reshape_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,672</span> │ conv2d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ digitcaps           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,474,560</span> │ lambda_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CapsuleLayer</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_23      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ digitcaps[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ input_layer_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,432</span> │ lambda_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dense_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ digitcaps[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">803,600</span> │ dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,215,568\u001b[0m (31.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,215,568</span> (31.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,215,568\u001b[0m (31.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,215,568</span> (31.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the AE-CNN model...\n",
            "Epoch 1/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.7534 - loss: 0.6738 - val_accuracy: 0.8744 - val_loss: 0.3353\n",
            "Epoch 2/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8905 - loss: 0.2990 - val_accuracy: 0.9001 - val_loss: 0.2767\n",
            "Epoch 3/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9085 - loss: 0.2460 - val_accuracy: 0.9110 - val_loss: 0.2422\n",
            "Epoch 4/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9197 - loss: 0.2124 - val_accuracy: 0.9145 - val_loss: 0.2333\n",
            "Epoch 5/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9332 - loss: 0.1810 - val_accuracy: 0.9223 - val_loss: 0.2145\n",
            "Epoch 6/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9423 - loss: 0.1573 - val_accuracy: 0.9225 - val_loss: 0.2161\n",
            "Epoch 7/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9481 - loss: 0.1399 - val_accuracy: 0.9172 - val_loss: 0.2265\n",
            "Epoch 8/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9564 - loss: 0.1196 - val_accuracy: 0.9235 - val_loss: 0.2316\n",
            "Epoch 9/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9637 - loss: 0.0965 - val_accuracy: 0.9250 - val_loss: 0.2342\n",
            "Epoch 10/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9693 - loss: 0.0812 - val_accuracy: 0.9200 - val_loss: 0.2991\n",
            "Training the Capsule Network model...\n",
            "Epoch 1/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 52ms/step - dense_31_loss: 0.1398 - dense_31_mse: 0.1398 - lambda_18_accuracy: 0.5545 - lambda_18_loss: 1.2599 - loss: 1.2600 - val_dense_31_loss: 0.0824 - val_dense_31_mse: 0.0824 - val_lambda_18_accuracy: 0.8376 - val_lambda_18_loss: 0.5335 - val_loss: 0.5342\n",
            "Epoch 2/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 45ms/step - dense_31_loss: 0.0750 - dense_31_mse: 0.0750 - lambda_18_accuracy: 0.8506 - lambda_18_loss: 0.4900 - loss: 0.4900 - val_dense_31_loss: 0.0595 - val_dense_31_mse: 0.0596 - val_lambda_18_accuracy: 0.8620 - val_lambda_18_loss: 0.4448 - val_loss: 0.4452\n",
            "Epoch 3/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 46ms/step - dense_31_loss: 0.0578 - dense_31_mse: 0.0578 - lambda_18_accuracy: 0.8703 - lambda_18_loss: 0.3917 - loss: 0.3917 - val_dense_31_loss: 0.0551 - val_dense_31_mse: 0.0552 - val_lambda_18_accuracy: 0.8744 - val_lambda_18_loss: 0.3690 - val_loss: 0.3693\n",
            "Epoch 4/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - dense_31_loss: 0.0546 - dense_31_mse: 0.0546 - lambda_18_accuracy: 0.8859 - lambda_18_loss: 0.3386 - loss: 0.3386 - val_dense_31_loss: 0.0534 - val_dense_31_mse: 0.0534 - val_lambda_18_accuracy: 0.8788 - val_lambda_18_loss: 0.3621 - val_loss: 0.3626\n",
            "Epoch 5/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - dense_31_loss: 0.0530 - dense_31_mse: 0.0530 - lambda_18_accuracy: 0.8950 - lambda_18_loss: 0.3085 - loss: 0.3085 - val_dense_31_loss: 0.0523 - val_dense_31_mse: 0.0524 - val_lambda_18_accuracy: 0.8806 - val_lambda_18_loss: 0.3573 - val_loss: 0.3580\n",
            "Epoch 6/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - dense_31_loss: 0.0521 - dense_31_mse: 0.0521 - lambda_18_accuracy: 0.9043 - lambda_18_loss: 0.2903 - loss: 0.2904 - val_dense_31_loss: 0.0519 - val_dense_31_mse: 0.0519 - val_lambda_18_accuracy: 0.8847 - val_lambda_18_loss: 0.3428 - val_loss: 0.3435\n",
            "Epoch 7/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - dense_31_loss: 0.0518 - dense_31_mse: 0.0518 - lambda_18_accuracy: 0.9135 - lambda_18_loss: 0.2663 - loss: 0.2663 - val_dense_31_loss: 0.0518 - val_dense_31_mse: 0.0519 - val_lambda_18_accuracy: 0.8871 - val_lambda_18_loss: 0.3331 - val_loss: 0.3338\n",
            "Epoch 8/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - dense_31_loss: 0.0514 - dense_31_mse: 0.0514 - lambda_18_accuracy: 0.9204 - lambda_18_loss: 0.2458 - loss: 0.2459 - val_dense_31_loss: 0.0519 - val_dense_31_mse: 0.0519 - val_lambda_18_accuracy: 0.8895 - val_lambda_18_loss: 0.3508 - val_loss: 0.3513\n",
            "Epoch 9/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - dense_31_loss: 0.0514 - dense_31_mse: 0.0514 - lambda_18_accuracy: 0.9237 - lambda_18_loss: 0.2396 - loss: 0.2396 - val_dense_31_loss: 0.0516 - val_dense_31_mse: 0.0517 - val_lambda_18_accuracy: 0.8923 - val_lambda_18_loss: 0.3404 - val_loss: 0.3411\n",
            "Epoch 10/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - dense_31_loss: 0.0514 - dense_31_mse: 0.0514 - lambda_18_accuracy: 0.9301 - lambda_18_loss: 0.2237 - loss: 0.2237 - val_dense_31_loss: 0.0517 - val_dense_31_mse: 0.0517 - val_lambda_18_accuracy: 0.8914 - val_lambda_18_loss: 0.3344 - val_loss: 0.3353\n",
            "Epoch 11/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - dense_31_loss: 0.0514 - dense_31_mse: 0.0514 - lambda_18_accuracy: 0.9360 - lambda_18_loss: 0.2069 - loss: 0.2070 - val_dense_31_loss: 0.0516 - val_dense_31_mse: 0.0516 - val_lambda_18_accuracy: 0.8868 - val_lambda_18_loss: 0.3496 - val_loss: 0.3503\n",
            "Epoch 12/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - dense_31_loss: 0.0513 - dense_31_mse: 0.0513 - lambda_18_accuracy: 0.9400 - lambda_18_loss: 0.1978 - loss: 0.1978 - val_dense_31_loss: 0.0515 - val_dense_31_mse: 0.0516 - val_lambda_18_accuracy: 0.8877 - val_lambda_18_loss: 0.3492 - val_loss: 0.3502\n",
            "AE-CNN history keys: ['accuracy', 'loss', 'val_accuracy', 'val_loss']\n",
            "CapsNet history keys: ['dense_31_loss', 'dense_31_mse', 'lambda_18_accuracy', 'lambda_18_loss', 'loss', 'val_dense_31_loss', 'val_dense_31_mse', 'val_lambda_18_accuracy', 'val_lambda_18_loss', 'val_loss']\n",
            "\n",
            "Evaluating AE-CNN model on test set...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9178 - loss: 0.2366\n",
            "AE-CNN Test accuracy: 0.9161\n",
            "\n",
            "Evaluating Capsule Network model on test set...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - dense_31_loss: 0.0517 - dense_31_mse: 0.0517 - lambda_18_accuracy: 0.8798 - lambda_18_loss: 0.3481 - loss: 0.3482\n",
            "Capsule Network Test accuracy: 0.0517\n",
            "Generating predictions...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step\n",
            "\n",
            "Overall Performance Metrics:\n",
            "                 Test Accuracy (%)  Precision (Macro) (%)  Recall (Macro) (%)  \\\n",
            "AE-CNN                       91.61              91.556572               91.61   \n",
            "Capsule Network              88.10              88.011367               88.10   \n",
            "\n",
            "                 F1 Score (Macro) (%)  \n",
            "AE-CNN                      91.568688  \n",
            "Capsule Network             87.981059  \n",
            "\n",
            "AE-CNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top       0.87      0.87      0.87      1000\n",
            "     Trouser       0.99      0.98      0.99      1000\n",
            "    Pullover       0.87      0.88      0.88      1000\n",
            "       Dress       0.91      0.92      0.91      1000\n",
            "        Coat       0.85      0.88      0.87      1000\n",
            "      Sandal       0.98      0.98      0.98      1000\n",
            "       Shirt       0.78      0.73      0.75      1000\n",
            "     Sneaker       0.95      0.98      0.96      1000\n",
            "         Bag       0.97      0.99      0.98      1000\n",
            "  Ankle boot       0.98      0.96      0.97      1000\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n",
            "\n",
            "Capsule Network Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top       0.80      0.87      0.83      1000\n",
            "     Trouser       0.99      0.97      0.98      1000\n",
            "    Pullover       0.77      0.84      0.80      1000\n",
            "       Dress       0.89      0.91      0.90      1000\n",
            "        Coat       0.82      0.76      0.79      1000\n",
            "      Sandal       0.98      0.96      0.97      1000\n",
            "       Shirt       0.70      0.61      0.65      1000\n",
            "     Sneaker       0.94      0.96      0.95      1000\n",
            "         Bag       0.96      0.97      0.96      1000\n",
            "  Ankle boot       0.96      0.96      0.96      1000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}